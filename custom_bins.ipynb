{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "SPARK_HOME=\"/opt/spark\"\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"python3\"\n",
    "# os.environ[\"PYSPARK_PYTHON\"]=\"python\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"]=\"jupyter\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON_OPTS\"]=\"notebook\"\n",
    "os.environ[\"SPARK_HOME\"] = SPARK_HOME\n",
    "os.environ[\"SPARK_CLASSPATH\"] = \"/opt/spark/jars/sqljdbc4.jar\"\n",
    "\n",
    " \n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"Task_3\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pyspark.sql import SQLContext\n",
    "# sqlContext = SQLContext(spark)\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- email: string (nullable = true)\n",
      "\n",
      "+--------------------+\n",
      "|               email|\n",
      "+--------------------+\n",
      "|rupeshrupees@gmai...|\n",
      "|    kokojambo@mon.eu|\n",
      "|         lol@fsa.org|\n",
      "|         ii@koko.com|\n",
      "|rupesh.subramani@...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_path = \"/home/rupeshr/Desktop/TSA_Python/sample_email.csv\"\n",
    "df =spark.read.format(\"com.databricks.spark.csv\")\\\n",
    "                    .option(\"multiline\", \"true\")\\\n",
    "                    .option(\"header\", \"true\")\\\n",
    "                    .option(\"inferSchema\", \"true\")\\\n",
    "                    .option(\"sep\", ',')\\\n",
    "                    .option('escape', '\\\"')\\\n",
    "                    .option(\"allowSingleQuotes\", \"true\")\\\n",
    "                    .option(\"ignoreLeadingWhiteSpace\", \"true\")\\\n",
    "                    .option(\"ignoreTrailingWhiteSpace\", \"true\")\\\n",
    "                    .load(df_path)\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import *\n",
    "from pyspark.sql.functions import UserDefinedFunction, udf\n",
    "from pyspark.sql.functions import lit\n",
    "import ast\n",
    "import numpy as np\n",
    "import traceback\n",
    "from math import ceil,floor\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------+---------+-------------------+\n",
      "|SystemCodeNumber|Capacity|Occupancy|        LastUpdated|\n",
      "+----------------+--------+---------+-------------------+\n",
      "|     BHMBCCMKT01|     577|       61|2016-10-04 07:59:42|\n",
      "|     BHMBCCMKT01|     577|       64|2016-10-04 08:25:42|\n",
      "|     BHMBCCMKT01|     577|       80|2016-10-04 08:59:42|\n",
      "|     BHMBCCMKT01|     577|      107|2016-10-04 09:32:46|\n",
      "|     BHMBCCMKT01|     577|      150|2016-10-04 09:59:48|\n",
      "|     BHMBCCMKT01|     577|      177|2016-10-04 10:26:49|\n",
      "|     BHMBCCMKT01|     577|      219|2016-10-04 10:59:48|\n",
      "|     BHMBCCMKT01|     577|     null|2016-10-04 11:25:47|\n",
      "|     BHMBCCMKT01|     577|     null|2016-10-04 11:59:44|\n",
      "|     BHMBCCMKT01|     577|     null|2016-10-04 12:29:45|\n",
      "|     BHMBCCMKT01|     577|     null|2016-10-04 13:02:48|\n",
      "|     BHMBCCMKT01|     577|     null|2016-10-04 13:29:45|\n",
      "|     BHMBCCMKT01|     577|     null|2016-10-04 14:02:47|\n",
      "|     BHMBCCMKT01|     577|     null|2016-10-04 14:29:49|\n",
      "|     BHMBCCMKT01|     577|     null|2016-10-04 14:57:13|\n",
      "|     BHMBCCMKT01|     577|      165|2016-10-04 15:30:14|\n",
      "|     BHMBCCMKT01|     577|      162|2016-10-04 16:04:12|\n",
      "|     BHMBCCMKT01|     577|      143|2016-10-04 16:31:14|\n",
      "|     BHMBCCMKT01|     577|       54|2016-10-05 07:57:17|\n",
      "|     BHMBCCMKT01|     577|       59|2016-10-05 08:30:15|\n",
      "+----------------+--------+---------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_path =\"/home/rupeshr/Desktop/TSA_Python/dataset/dataset.csv\"\n",
    "df = spark.read.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(df_path)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Cannot resolve column name \"Occupany\" among (SystemCodeNumber, Capacity, Occupancy, LastUpdated);",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-104-b5848ceba904>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Occupancy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Occupany'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmaxi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmini\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mvals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmini\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/group.py\u001b[0m in \u001b[0;36m_api\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_api\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jgd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m                 \u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Cannot resolve column name \"Occupany\" among (SystemCodeNumber, Capacity, Occupancy, LastUpdated);"
     ]
    }
   ],
   "source": [
    "val = df.groupby().max('Occupancy').collect()[0]\n",
    "vals = df.groupby().min('Occupany').collect()[0]\n",
    "maxi = [val[0]]\n",
    "mini = [vals[0]]\n",
    "print(maxi, mini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18, 20, 30, 40, 50, 60, 70, 70]\n",
      "[18, 20, 30, 40, 50, 60, 70]\n"
     ]
    }
   ],
   "source": [
    "x=[20,30,40,50,60,70]\n",
    "s= mini + x + maxi\n",
    "print(s)\n",
    "s=list(set(s))\n",
    "s.sort(reverse=False)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucketizer = Bucketizer(splits=s, inputCol=\"Age\",outputCol=\"age_result\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+---+------------------+----------------------+----------+\n",
      "|CustomerID| Genre|Age|Annual Income (k$)|Spending Score (1-100)|age_result|\n",
      "+----------+------+---+------------------+----------------------+----------+\n",
      "|         1|  Male| 19|                15|                    39|       0.0|\n",
      "|         2|  Male| 21|                15|                    81|       1.0|\n",
      "|         3|Female| 20|                16|                     6|       1.0|\n",
      "|         4|Female| 23|                16|                    77|       1.0|\n",
      "|         5|Female| 31|                17|                    40|       2.0|\n",
      "|         6|Female| 22|                17|                    76|       1.0|\n",
      "|         7|Female| 35|                18|                     6|       2.0|\n",
      "|         8|Female| 23|                18|                    94|       1.0|\n",
      "|         9|  Male| 64|                19|                     3|       5.0|\n",
      "|        10|Female| 30|                19|                    72|       2.0|\n",
      "|        11|  Male| 67|                19|                    14|       5.0|\n",
      "|        12|Female| 35|                19|                    99|       2.0|\n",
      "|        13|Female| 58|                20|                    15|       4.0|\n",
      "|        14|Female| 24|                20|                    77|       1.0|\n",
      "|        15|  Male| 37|                20|                    13|       2.0|\n",
      "|        16|  Male| 22|                20|                    79|       1.0|\n",
      "|        17|Female| 35|                21|                    35|       2.0|\n",
      "|        18|  Male| 20|                21|                    66|       1.0|\n",
      "|        19|  Male| 52|                23|                    29|       4.0|\n",
      "|        20|Female| 35|                23|                    98|       2.0|\n",
      "+----------+------+---+------------------+----------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bucketed = bucketizer.transform(df)\n",
    "bucketed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(18, 20), (20, 30), (30, 40), (40, 50), (50, 60), (60, 70)]\n"
     ]
    }
   ],
   "source": [
    "y = []\n",
    "for i in range(len(s[:-1])):\n",
    "    q = (np.round(s[i],6), np.round(s[i+1],6))\n",
    "    y.append(q)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(s[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testUDF(col, y):\n",
    "    if col:\n",
    "        y = ast.literal_eval(str(y))\n",
    "        return str(y[int(col)])\n",
    "    elif '0' in str(col):\n",
    "        return str(ast.literal_eval(str(y))[0])\n",
    "    else:\n",
    "        return None\n",
    "myudf = udf(testUDF, StringType())\n",
    "f = bucketed.withColumn('age_source', myudf('age_result', lit(str(y))))\n",
    "sd = f.drop(\"age_result\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(18, 20), (20, 30), (30, 40), (40, 50), (50, 60), (60, 70)]\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18, 20)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ast.literal_eval(str(y))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18, 20)\n"
     ]
    }
   ],
   "source": [
    "print(y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+---+------------------+----------------------+----------+\n",
      "|CustomerID| Genre|Age|Annual Income (k$)|Spending Score (1-100)|age_source|\n",
      "+----------+------+---+------------------+----------------------+----------+\n",
      "|         1|  Male| 19|                15|                    39|  (18, 20)|\n",
      "|         2|  Male| 21|                15|                    81|  (20, 30)|\n",
      "|         3|Female| 20|                16|                     6|  (20, 30)|\n",
      "|         4|Female| 23|                16|                    77|  (20, 30)|\n",
      "|         5|Female| 31|                17|                    40|  (30, 40)|\n",
      "|         6|Female| 22|                17|                    76|  (20, 30)|\n",
      "|         7|Female| 35|                18|                     6|  (30, 40)|\n",
      "|         8|Female| 23|                18|                    94|  (20, 30)|\n",
      "|         9|  Male| 64|                19|                     3|  (60, 70)|\n",
      "|        10|Female| 30|                19|                    72|  (30, 40)|\n",
      "|        11|  Male| 67|                19|                    14|  (60, 70)|\n",
      "|        12|Female| 35|                19|                    99|  (30, 40)|\n",
      "|        13|Female| 58|                20|                    15|  (50, 60)|\n",
      "|        14|Female| 24|                20|                    77|  (20, 30)|\n",
      "|        15|  Male| 37|                20|                    13|  (30, 40)|\n",
      "|        16|  Male| 22|                20|                    79|  (20, 30)|\n",
      "|        17|Female| 35|                21|                    35|  (30, 40)|\n",
      "|        18|  Male| 20|                21|                    66|  (20, 30)|\n",
      "|        19|  Male| 52|                23|                    29|  (50, 60)|\n",
      "|        20|Female| 35|                23|                    98|  (30, 40)|\n",
      "+----------+------+---+------------------+----------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sd.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+---+------------------+----------------------+----------+\n",
      "|CustomerID| Genre|Age|Annual Income (k$)|Spending Score (1-100)|age_source|\n",
      "+----------+------+---+------------------+----------------------+----------+\n",
      "|         1|  Male| 19|                15|                    39|      null|\n",
      "|         2|  Male| 21|                15|                    81|  (20, 30)|\n",
      "|         3|Female| 20|                16|                     6|  (20, 30)|\n",
      "|         4|Female| 23|                16|                    77|  (20, 30)|\n",
      "|         5|Female| 31|                17|                    40|  (30, 40)|\n",
      "|         6|Female| 22|                17|                    76|  (20, 30)|\n",
      "|         7|Female| 35|                18|                     6|  (30, 40)|\n",
      "|         8|Female| 23|                18|                    94|  (20, 30)|\n",
      "|         9|  Male| 64|                19|                     3|  (60, 70)|\n",
      "|        10|Female| 30|                19|                    72|  (30, 40)|\n",
      "|        11|  Male| 67|                19|                    14|  (60, 70)|\n",
      "|        12|Female| 35|                19|                    99|  (30, 40)|\n",
      "|        13|Female| 58|                20|                    15|  (50, 60)|\n",
      "|        14|Female| 24|                20|                    77|  (20, 30)|\n",
      "|        15|  Male| 37|                20|                    13|  (30, 40)|\n",
      "|        16|  Male| 22|                20|                    79|  (20, 30)|\n",
      "|        17|Female| 35|                21|                    35|  (30, 40)|\n",
      "|        18|  Male| 20|                21|                    66|  (20, 30)|\n",
      "|        19|  Male| 52|                23|                    29|  (50, 60)|\n",
      "|        20|Female| 35|                23|                    98|  (30, 40)|\n",
      "+----------+------+---+------------------+----------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sd.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_binning(data, column, values, new_column):\n",
    "    try:\n",
    "        if data.count() == 1:\n",
    "            columnValue = data.select(column).collect()[0][0]\n",
    "            data = data.withColumn(new_column, lit(str(str(columnValue))))\n",
    "            return data\n",
    "        else:\n",
    "            val = data.groupby().max(column[0]).collect()[0]\n",
    "            vals = data.groupby().min(column[0]).collect()[0]\n",
    "            c = [val[0]] # max\n",
    "            d = [vals[0]] # min\n",
    "            s= d + values + c\n",
    "            print(s)\n",
    "            s=list(set(s))\n",
    "            s.sort(reverse=False)\n",
    "            print(s)\n",
    "            bucketizer = Bucketizer(splits=s, inputCol=column[0],outputCol='temp5_binning')\n",
    "            bucketed = bucketizer.transform(data)\n",
    "            y = []\n",
    "            for i in range(len(s[:-1])):\n",
    "                q = (np.round(s[i],6), np.round(s[i+1],6))\n",
    "                y.append(q)\n",
    "            print(y)\n",
    "            def testUDF(col, y):\n",
    "                if col:\n",
    "                    y = ast.literal_eval(str(y))\n",
    "                    return str(y[int(col)])\n",
    "                elif '0' in str(col):\n",
    "                    return str(ast.literal_eval(str(y))[0])\n",
    "                else:\n",
    "                    return None\n",
    "            myudf = udf(testUDF, StringType())\n",
    "            x1 = bucketed.withColumn(new_column, myudf(\"temp5_binning\", lit(str(y))))\n",
    "            x1 = x1.drop(\"temp5_binning\")\n",
    "            return x1\n",
    "    except Exception as e:\n",
    "        traceback.print_exc()\n",
    "        # print(e)\n",
    "        return \"error\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-8, 200, 500, 800, 1000, 1500, 2000, 4327]\n",
      "[-8, 200, 500, 800, 1000, 1500, 2000, 4327]\n",
      "[(-8, 200), (200, 500), (500, 800), (800, 1000), (1000, 1500), (1500, 2000), (2000, 4327)]\n",
      "+----------------+--------+---------+-------------------+----------+\n",
      "|SystemCodeNumber|Capacity|Occupancy|        LastUpdated|Occ_source|\n",
      "+----------------+--------+---------+-------------------+----------+\n",
      "|     BHMBCCMKT01|     577|       61|2016-10-04 07:59:42| (-8, 200)|\n",
      "|     BHMBCCMKT01|     577|       64|2016-10-04 08:25:42| (-8, 200)|\n",
      "|     BHMBCCMKT01|     577|       80|2016-10-04 08:59:42| (-8, 200)|\n",
      "|     BHMBCCMKT01|     577|      107|2016-10-04 09:32:46| (-8, 200)|\n",
      "|     BHMBCCMKT01|     577|      150|2016-10-04 09:59:48| (-8, 200)|\n",
      "|     BHMBCCMKT01|     577|      177|2016-10-04 10:26:49| (-8, 200)|\n",
      "|     BHMBCCMKT01|     577|      219|2016-10-04 10:59:48|(200, 500)|\n",
      "|     BHMBCCMKT01|     577|     null|2016-10-04 11:25:47|      null|\n",
      "|     BHMBCCMKT01|     577|     null|2016-10-04 11:59:44|      null|\n",
      "|     BHMBCCMKT01|     577|     null|2016-10-04 12:29:45|      null|\n",
      "|     BHMBCCMKT01|     577|     null|2016-10-04 13:02:48|      null|\n",
      "|     BHMBCCMKT01|     577|     null|2016-10-04 13:29:45|      null|\n",
      "|     BHMBCCMKT01|     577|     null|2016-10-04 14:02:47|      null|\n",
      "|     BHMBCCMKT01|     577|     null|2016-10-04 14:29:49|      null|\n",
      "|     BHMBCCMKT01|     577|     null|2016-10-04 14:57:13|      null|\n",
      "|     BHMBCCMKT01|     577|      165|2016-10-04 15:30:14| (-8, 200)|\n",
      "|     BHMBCCMKT01|     577|      162|2016-10-04 16:04:12| (-8, 200)|\n",
      "|     BHMBCCMKT01|     577|      143|2016-10-04 16:31:14| (-8, 200)|\n",
      "|     BHMBCCMKT01|     577|       54|2016-10-05 07:57:17| (-8, 200)|\n",
      "|     BHMBCCMKT01|     577|       59|2016-10-05 08:30:15| (-8, 200)|\n",
      "+----------------+--------+---------+-------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s=custom_binning(df,['Occupancy'],[200,500,800,1000,1500,2000],'Occ_source')\n",
    "s.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "int() argument must be a string, a bytes-like object or a number, not 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-100-0789f71f52e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'1'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a number, not 'list'"
     ]
    }
   ],
   "source": [
    "s=['1','2']\n",
    "print(int(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
