{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "SPARK_HOME=\"/opt/spark\"\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"python3\"\n",
    "# os.environ[\"PYSPARK_PYTHON\"]=\"python\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"]=\"jupyter\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON_OPTS\"]=\"notebook\"\n",
    "os.environ[\"SPARK_HOME\"] = SPARK_HOME\n",
    "os.environ[\"SPARK_CLASSPATH\"] = \"/opt/spark/jars/sqljdbc4.jar\"\n",
    "\n",
    " \n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"Task_3\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pyspark.sql import SQLContext\n",
    "# sqlContext = SQLContext(spark)\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Money: double (nullable = true)\n",
      " |-- Spending: double (nullable = true)\n",
      " |-- Gain: integer (nullable = true)\n",
      " |-- Loss: integer (nullable = true)\n",
      "\n",
      "+----------+------+--------+----+----+\n",
      "|      Date| Money|Spending|Gain|Loss|\n",
      "+----------+------+--------+----+----+\n",
      "|1995-01-01|3492.4|  4851.2|2891|1000|\n",
      "|1995-02-01|3489.9|  4850.8|2893|1101|\n",
      "|1995-03-01|3491.1|  4885.4|2895|1202|\n",
      "|1995-04-01|3499.2|  4890.2|2897|1303|\n",
      "|1995-05-01|3524.2|  4933.1|   0|1404|\n",
      "|1995-06-01|3548.9|  4977.5|   0|1505|\n",
      "|1995-07-01|3567.4|  4970.2|   0|1606|\n",
      "|1995-08-01|3589.0|  5005.3|   0|1707|\n",
      "|1995-09-01|3602.1|  5020.5|   0|1808|\n",
      "|1995-10-01|3613.4|  5013.9|   0|1909|\n",
      "|1995-11-01|3619.9|  5055.6|   0|2010|\n",
      "|1995-12-01|3629.5|  5097.5|   0|2111|\n",
      "|1996-01-01|3647.9|  5085.7|   0|2212|\n",
      "|1996-02-01|3661.9|  5132.8|2917|2313|\n",
      "|1996-03-01|3687.0|  5173.3|2919|2414|\n",
      "|1996-04-01|3697.8|  5208.0|2921|   0|\n",
      "|1996-05-01|3709.7|  5223.8|2923|   0|\n",
      "|1996-06-01|3722.7|  5229.8|2925|   0|\n",
      "|1996-07-01|3737.3|  5251.9|2927|   0|\n",
      "|1996-08-01|3744.3|  5275.0|2929|   0|\n",
      "+----------+------+--------+----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_path = \"/home/rupeshr/Desktop/TSA_Python/dataset/multivariate_dataset.csv\"\n",
    "df =spark.read.format(\"com.databricks.spark.csv\")\\\n",
    "                    .option(\"multiline\", \"true\")\\\n",
    "                    .option(\"header\", \"true\")\\\n",
    "                    .option(\"inferSchema\", \"true\")\\\n",
    "                    .option(\"sep\", ',')\\\n",
    "                    .option('escape', '\\\"')\\\n",
    "                    .option(\"allowSingleQuotes\", \"true\")\\\n",
    "                    .option(\"ignoreLeadingWhiteSpace\", \"true\")\\\n",
    "                    .option(\"ignoreTrailingWhiteSpace\", \"true\")\\\n",
    "                    .load(df_path)\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-----------------+------------------+------------------+------------------+\n",
      "|summary|      Date|            Money|          Spending|              Gain|              Loss|\n",
      "+-------+----------+-----------------+------------------+------------------+------------------+\n",
      "|  count|       252|              252|               252|               252|               252|\n",
      "|   mean|      null|6978.151190476192| 8562.311904761906|3038.1785714285716|13584.436507936507|\n",
      "| stddev|      null|2554.325114763373|2231.9333524052263| 602.0224313311252| 7511.739127860896|\n",
      "|    min|1995-01-01|           3489.9|            4850.8|                 0|                 0|\n",
      "|    max|2015-12-01|          12335.9|           12469.1|              3393|             26351|\n",
      "+-------+----------+-----------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_fill(username,dsname, column,value=None): # auto fill new\n",
    "    sqlContext = spark_session(username)\n",
    "    for i in dsname.columns:\n",
    "        if i in column:\n",
    "            s = dsname.select(i).dtypes[0][1]\n",
    "            if s == \"int\" or s == \"float\" or s == \"double\" or s == \"bigint\" or s == \"long\":\n",
    "                m = \"numerical\"\n",
    "                try:\n",
    "                    #query = \"kurtosis(\" + i + \") as kurtosis_\" + \", skewness(\" + i + \") as skewness_\" + i + \",\"\n",
    "                    #query = query.strip()[:-1]\n",
    "                    #dsname.registerTempTable(\"tempTable\")\n",
    "                    #query = \"SELECT \" + query + \" from tempTable\"\n",
    "                    #stats = sqlContext.sql(query).collect()\n",
    "                    mean = dsname.select(f.avg(i)).collect()[0]\n",
    "                    mean = mean.asDict()\n",
    "                    s = \"avg(%s)\" % i\n",
    "                    mean = mean[s]\n",
    "                    median = dsname.approxQuantile(i, [0.5], 0)\n",
    "                    median = median[0]\n",
    "                    if stats[0][1] > - 0.5 and stats[0][1] < 0.5:\n",
    "                        dsname = dsname.na.fill(mean, i)\n",
    "                    else:\n",
    "                        dsname = dsname.na.fill(median, i)\n",
    "                except:\n",
    "                    dsname = dsname.na.fill(0, i)\n",
    "\n",
    "            elif s == \"string\":\n",
    "                m = \"categorical\"\n",
    "                try:\n",
    "                    dsname = dsname.withColumn(i, f.trim(f.col(i)))\n",
    "                    if (i == '!' or i == '@' or i == '#' or i == '$' or i == '&' or i == '*' or i == '_' or i == '-' or i == 'NA' or i == '?'):\n",
    "                        dsname = dsname.withColumn(i, f.when((f.col(i) == \"!\") | (f.col(i) == \"@\") | (f.col(i) == '#') | (f.col(i) == '$') | (f.col(i) == '&') | (f.col(i) == '*' | (f.col(i) == '_') | (f.col(i) == '-') | (f.col(i) == 'NA') | (f.col(i) == '?')), None).otherwise(f.col(i)))\n",
    "                    dsname.registerTempTable(\"tempTable\")\n",
    "                    modeval = dsname.groupBy(i).count().orderBy(f.desc(\"count\")).collect()\n",
    "                    temp_iterator = 0\n",
    "                    temp = modeval[temp_iterator][0]\n",
    "                    mode = temp\n",
    "                    if mode == None:\n",
    "                        mode2 = modeval[1][0]\n",
    "                        dsname = dsname.na.fill(mode2, i)\n",
    "                    else:\n",
    "                        dsname = dsname.na.fill(mode, i)\n",
    "                except:\n",
    "                    dsname = dsname.na.fill(\"0\", i)\n",
    "    return dsname,m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_fill(*,df,column,method,value=None):# auto fill new\n",
    "    \n",
    "    if value is not None:\n",
    "        value=value\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    for i in df.columns:\n",
    "        if i in column:\n",
    "            s = df.select(i).dtypes[0]\n",
    "            print(s)\n",
    "            if s[1] == \"int\" or s[1] == \"float\" or s[1] == \"double\" or s[1] == \"bigint\" or s[1] == \"long\":\n",
    "                m='numerical'\n",
    "                try:\n",
    "                    if method=='mean':\n",
    "                        mean = df.select(avg(i)).collect()[0]\n",
    "                        print(mean)\n",
    "                        mean = mean.asDict()\n",
    "                        s = \"avg(%s)\" % i\n",
    "                        mean = mean[s]\n",
    "                        dsname = df.na.fill(mean, i)\n",
    "                    elif method =='median':\n",
    "                        median = df.approxQuantile(i, [0.5], 0)\n",
    "                        median = median[0]\n",
    "                        dsname = df.na.fill(median, i)\n",
    "                    elif method =='custom':\n",
    "                        dsname = df.na.fill(value, i)\n",
    "                except:\n",
    "                    dsname = df.na.fill(0, i)\n",
    "\n",
    "            elif s[1] == \"string\":\n",
    "                m = \"categorical\"\n",
    "                try:\n",
    "                    if method =='mode':\n",
    "                        modeval=df.groupBy(i).count().orderBy(desc(\"count\")).collect()\n",
    "                        temp_iterator = 0\n",
    "                        temp = modeval[temp_iterator][0]\n",
    "                        while(temp is None):\n",
    "                            temp_iterator +=1\n",
    "                            temp = modeval[temp_iterator][0]\n",
    "                        mode = temp\n",
    "                        dsname = df.na.fill(mode, i)\n",
    "                    elif method =='custom':\n",
    "                        dsname = df.na.fill(value, i)\n",
    "                except:\n",
    "                    dsname = df.na.fill(\"0\", i)\n",
    "\n",
    "    return dsname,m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+-----+\n",
      "|      Date|category|count|\n",
      "+----------+--------+-----+\n",
      "|1996-01-01|   shade|   23|\n",
      "|1996-01-02|     sun|   21|\n",
      "|1996-01-03|     sun| null|\n",
      "|1996-01-04|    null| null|\n",
      "|1996-01-05|   shade|   20|\n",
      "|1996-01-06|     sun| null|\n",
      "|1996-01-07|    null| null|\n",
      "|1996-01-08|   shade|   15|\n",
      "|1996-01-09|     sun|   14|\n",
      "|1996-01-10|   shade|   17|\n",
      "|1996-01-11|    null|   12|\n",
      "|1996-01-12|     sun| null|\n",
      "|1996-01-13|    null|   11|\n",
      "|1996-01-14|   shade| null|\n",
      "|1996-01-15|     sun| null|\n",
      "+----------+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_path = \"/home/rupeshr/Desktop/TSA_Python/categorydataset.csv\"\n",
    "df = spark.read.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(df_path)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('category', 'string')\n",
      "+----------+--------+-----+\n",
      "|      Date|category|count|\n",
      "+----------+--------+-----+\n",
      "|1996-01-01|   shade|   23|\n",
      "|1996-01-02|     sun|   21|\n",
      "|1996-01-03|     sun| null|\n",
      "|1996-01-04|   shade| null|\n",
      "|1996-01-05|   shade|   20|\n",
      "|1996-01-06|     sun| null|\n",
      "|1996-01-07|   shade| null|\n",
      "|1996-01-08|   shade|   15|\n",
      "|1996-01-09|     sun|   14|\n",
      "|1996-01-10|   shade|   17|\n",
      "|1996-01-11|   shade|   12|\n",
      "|1996-01-12|     sun| null|\n",
      "|1996-01-13|   shade|   11|\n",
      "|1996-01-14|   shade| null|\n",
      "|1996-01-15|     sun| null|\n",
      "+----------+--------+-----+\n",
      "\n",
      "categorical\n"
     ]
    }
   ],
   "source": [
    "df1,m=auto_fill(df=df,column='category',method='custom',value='shade')\n",
    "df1.show()\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(avg(count)=16.625)\n"
     ]
    }
   ],
   "source": [
    "mean = df.select(avg('count')).collect()[0]\n",
    "print(mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'avg(count)': 16.625}\n"
     ]
    }
   ],
   "source": [
    "mean = mean.asDict()\n",
    "print(mean)                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg(count)\n"
     ]
    }
   ],
   "source": [
    "s = \"avg(%s)\" % 'count'\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.625\n"
     ]
    }
   ],
   "source": [
    "mean = mean[s]\n",
    "print(mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[Date: string, category: string, count: int]\n"
     ]
    }
   ],
   "source": [
    "dsname = df.na.fill(mean, 'count')\n",
    "print(dsname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+-----+\n",
      "|      Date|category|count|\n",
      "+----------+--------+-----+\n",
      "|1996-01-01|   shade|   23|\n",
      "|1996-01-02|     sun|   21|\n",
      "|1996-01-03|     sun|   16|\n",
      "|1996-01-04|    null|   16|\n",
      "|1996-01-05|   shade|   20|\n",
      "|1996-01-06|     sun|   16|\n",
      "|1996-01-07|    null|   16|\n",
      "|1996-01-08|   shade|   15|\n",
      "|1996-01-09|     sun|   14|\n",
      "|1996-01-10|   shade|   17|\n",
      "|1996-01-11|    null|   12|\n",
      "|1996-01-12|     sun|   16|\n",
      "|1996-01-13|    null|   11|\n",
      "|1996-01-14|   shade|   16|\n",
      "|1996-01-15|     sun|   16|\n",
      "+----------+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dsname.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
