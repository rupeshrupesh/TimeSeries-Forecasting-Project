{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "SPARK_HOME=\"/opt/spark\"\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"python3\"\n",
    "# os.environ[\"PYSPARK_PYTHON\"]=\"python\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"]=\"jupyter\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON_OPTS\"]=\"notebook\"\n",
    "os.environ[\"SPARK_HOME\"] = SPARK_HOME\n",
    "os.environ[\"SPARK_CLASSPATH\"] = \"/opt/spark/jars/sqljdbc4.jar\"\n",
    "\n",
    " \n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"Task_3\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pyspark.sql import SQLContext\n",
    "# sqlContext = SQLContext(spark)\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Money: double (nullable = true)\n",
      " |-- Spending: double (nullable = true)\n",
      " |-- Gain: integer (nullable = true)\n",
      " |-- Loss: integer (nullable = true)\n",
      "\n",
      "+----------+------+--------+----+----+\n",
      "|      Date| Money|Spending|Gain|Loss|\n",
      "+----------+------+--------+----+----+\n",
      "|1995-01-01|3492.4|  4851.2|2891|1000|\n",
      "|1995-02-01|3489.9|  4850.8|2893|1101|\n",
      "|1995-03-01|3491.1|  4885.4|2895|1202|\n",
      "|1995-04-01|3499.2|  4890.2|2897|1303|\n",
      "|1995-05-01|3524.2|  4933.1|   0|1404|\n",
      "|1995-06-01|3548.9|  4977.5|   0|1505|\n",
      "|1995-07-01|3567.4|  4970.2|   0|1606|\n",
      "|1995-08-01|3589.0|  5005.3|   0|1707|\n",
      "|1995-09-01|3602.1|  5020.5|   0|1808|\n",
      "|1995-10-01|3613.4|  5013.9|   0|1909|\n",
      "|1995-11-01|3619.9|  5055.6|   0|2010|\n",
      "|1995-12-01|3629.5|  5097.5|   0|2111|\n",
      "|1996-01-01|3647.9|  5085.7|   0|2212|\n",
      "|1996-02-01|3661.9|  5132.8|2917|2313|\n",
      "|1996-03-01|3687.0|  5173.3|2919|2414|\n",
      "|1996-04-01|3697.8|  5208.0|2921|   0|\n",
      "|1996-05-01|3709.7|  5223.8|2923|   0|\n",
      "|1996-06-01|3722.7|  5229.8|2925|   0|\n",
      "|1996-07-01|3737.3|  5251.9|2927|   0|\n",
      "|1996-08-01|3744.3|  5275.0|2929|   0|\n",
      "+----------+------+--------+----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_path = \"/home/rupeshr/Desktop/TSA_Python/dataset/multivariate_dataset.csv\"\n",
    "df =spark.read.format(\"com.databricks.spark.csv\")\\\n",
    "                    .option(\"multiline\", \"true\")\\\n",
    "                    .option(\"header\", \"true\")\\\n",
    "                    .option(\"inferSchema\", \"true\")\\\n",
    "                    .option(\"sep\", ',')\\\n",
    "                    .option('escape', '\\\"')\\\n",
    "                    .option(\"allowSingleQuotes\", \"true\")\\\n",
    "                    .option(\"ignoreLeadingWhiteSpace\", \"true\")\\\n",
    "                    .option(\"ignoreTrailingWhiteSpace\", \"true\")\\\n",
    "                    .load(df_path)\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+--------+----+----+\n",
      "|      Date| Money|Spending|Gain|Loss|\n",
      "+----------+------+--------+----+----+\n",
      "|1995-01-01|3492.4|  4851.2|2891|1000|\n",
      "|1995-02-01|3489.9|  4850.8|2893|1101|\n",
      "|1995-03-01|3491.1|  4885.4|2895|1202|\n",
      "|1995-04-01|3499.2|  4890.2|2897|1303|\n",
      "|1995-05-01|3524.2|  4933.1|2917|1404|\n",
      "|1995-06-01|3548.9|  4977.5|2917|1505|\n",
      "|1995-07-01|3567.4|  4970.2|2917|1606|\n",
      "|1995-08-01|3589.0|  5005.3|2917|1707|\n",
      "|1995-09-01|3602.1|  5020.5|2917|1808|\n",
      "|1995-10-01|3613.4|  5013.9|2917|1909|\n",
      "|1995-11-01|3619.9|  5055.6|2917|2010|\n",
      "|1995-12-01|3629.5|  5097.5|2917|2111|\n",
      "|1996-01-01|3647.9|  5085.7|2917|2212|\n",
      "|1996-02-01|3661.9|  5132.8|2917|2313|\n",
      "|1996-03-01|3687.0|  5173.3|2919|2414|\n",
      "|1996-04-01|3697.8|  5208.0|2921|   0|\n",
      "|1996-05-01|3709.7|  5223.8|2923|   0|\n",
      "|1996-06-01|3722.7|  5229.8|2925|   0|\n",
      "|1996-07-01|3737.3|  5251.9|2927|   0|\n",
      "|1996-08-01|3744.3|  5275.0|2929|   0|\n",
      "+----------+------+--------+----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Window\n",
    "# Auxiliary functions\n",
    "# Pandas Types -> Spark Types\n",
    "#from edge.stats.sparksession import spark_session\n",
    "\n",
    "def equivalent_type(f):\n",
    "    if f == 'datetime64[ns]': return DateType()\n",
    "    elif f == 'int64': return LongType()\n",
    "    elif f == 'int32': return IntegerType()\n",
    "    elif f == 'float64': return DoubleType()\n",
    "    else: return StringType()\n",
    "def define_structure(string, format_type):\n",
    "    try: typo = equivalent_type(format_type)\n",
    "    except: typo = StringType()\n",
    "    return StructField(string, typo)\n",
    "# Converts pandas dataframe to spark dataframe\n",
    "def pandas_to_spark(df_pandas):\n",
    "    sqlContext = spark\n",
    "    columns = list(df_pandas.columns)\n",
    "    types = list(df_pandas.dtypes)\n",
    "    struct_list = []\n",
    "    for column, typo in zip(columns, types):\n",
    "        struct_list.append(define_structure(column, typo))\n",
    "    p_schema = StructType(struct_list)\n",
    "    return sqlContext.createDataFrame(df_pandas, p_schema)\n",
    "\n",
    "\n",
    "def replace(df, column,replace,value=None): \n",
    "    for i in df.columns:\n",
    "        if i in column:\n",
    "            s = df.select(i).dtypes[0]\n",
    "            if s[1] == \"int\" or s[1] == \"float\" or s[1] == \"double\" or s[1] == \"bigint\" or s[1] == \"long\":\n",
    "                if replace =='mean':\n",
    "                    mean = df.select(avg(i)).collect()[0]\n",
    "                    mean = mean.asDict()\n",
    "                    s=\"avg(%s)\"%i\n",
    "                    mean = mean[s]\n",
    "                    df= df.na.replace(to_replace=0,value=mean,subset=i)\n",
    "                elif replace =='median':\n",
    "                    median = df.approxQuantile(i, [0.5], 0)\n",
    "                    median = median[0]\n",
    "                    df= df.na.replace(to_replace=0,value=median,subset=i)\n",
    "                elif replace =='max':\n",
    "                    maximum = df.select(max(i)).collect()[0]\n",
    "                    maximum = maximum.asDict()\n",
    "                    s=\"max(%s)\"%i\n",
    "                    maximum = maximum[s]\n",
    "                    df= df.na.replace(to_replace=0,value=maximum,subset=i)\n",
    "                elif replace =='min':\n",
    "                    rem_zero = df.filter((df[i] != 0))\n",
    "                    rem_zero=rem_zero.drop('index')\n",
    "                    minimum = rem_zero.select(min(i)).collect()[0]\n",
    "                    minimum = minimum.asDict()\n",
    "                    s=\"min(%s)\"%i\n",
    "                    minimum = minimum[s]\n",
    "                    df= df.na.replace(to_replace=0,value=minimum,subset=i)\n",
    "                    df=df.drop('index')\n",
    "                elif replace =='ffill':\n",
    "                    temp_df=df.select(i).toPandas()\n",
    "                    temp_df.replace(0,method='ffill',inplace=True)\n",
    "                    temp_df=pandas_to_spark(temp_df) \n",
    "                    temp_df=temp_df.select(col(i).alias('renamed'))\n",
    "                    w = Window.orderBy(monotonically_increasing_id())\n",
    "                    df = df.withColumn(\"index\", row_number().over(w))\n",
    "                    temp_df = temp_df.withColumn(\"index\", row_number().over(w))\n",
    "                    df= df.join(temp_df, df['index'] == temp_df['index'], 'left').drop(temp_df['index'])\n",
    "                    df=df.withColumn(i,col('renamed')).drop('index','renamed')\n",
    "                elif replace =='bfill':\n",
    "                    temp_df=df.select(i).toPandas()\n",
    "                    temp_df.replace(0,method='bfill',inplace=True)\n",
    "                    temp_df=pandas_to_spark(temp_df) \n",
    "                    temp_df=temp_df.select(col(i).alias('renamed'))\n",
    "                    w = Window.orderBy(monotonically_increasing_id())\n",
    "                    df = df.withColumn(\"index\", row_number().over(w))\n",
    "                    temp_df = temp_df.withColumn(\"index\", row_number().over(w))\n",
    "                    df= df.join(temp_df, df['index'] == temp_df['index'], 'left').drop(temp_df['index'])\n",
    "                    df=df.withColumn(i,col('renamed')).drop('index','renamed')\n",
    "                elif replace =='custom':\n",
    "                    df= df.na.replace(to_replace=0,value=value,subset=i)\n",
    "                    \n",
    "    return df\n",
    "\n",
    "df1=replace(df,'Gain','bfill')\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.DataFrame({\"Col1\": [10, 20, 15, 30, 45],\n",
    "                   \"Col2\": [13, 23, 18, 33, 48],\n",
    "                   \"Col3\": [17, 27, 22, 37, 52]},\n",
    "                  index=pd.date_range(\"2020-01-01\", \"2020-01-05\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
