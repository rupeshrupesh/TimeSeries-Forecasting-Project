{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "SPARK_HOME=\"/opt/spark\"\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"python3\"\n",
    "# os.environ[\"PYSPARK_PYTHON\"]=\"python\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"]=\"jupyter\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON_OPTS\"]=\"notebook\"\n",
    "os.environ[\"SPARK_HOME\"] = SPARK_HOME\n",
    "os.environ[\"SPARK_CLASSPATH\"] = \"/opt/spark/jars/sqljdbc4.jar\"\n",
    "\n",
    " \n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"new\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pyspark.sql import SQLContext\n",
    "# sqlContext = SQLContext(spark)\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- column: string (nullable = true)\n",
      "\n",
      "+------+\n",
      "|column|\n",
      "+------+\n",
      "|    12|\n",
      "|    14|\n",
      "|   abc|\n",
      "|   efg|\n",
      "|     2|\n",
      "|   qwe|\n",
      "| 12ews|\n",
      "|   aws|\n",
      "|    34|\n",
      "|    56|\n",
      "|  said|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_path = \"/home/rupeshr/Documents/split_Test.csv\"\n",
    "df =spark.read.format(\"com.databricks.spark.csv\")\\\n",
    "                    .option(\"multiline\", \"true\")\\\n",
    "                    .option(\"header\", \"true\")\\\n",
    "                    .option(\"inferSchema\", \"true\")\\\n",
    "                    .option(\"sep\", ',')\\\n",
    "                    .option('escape', '\\\"')\\\n",
    "                    .option(\"allowSingleQuotes\", \"true\")\\\n",
    "                    .option(\"ignoreLeadingWhiteSpace\", \"true\")\\\n",
    "                    .option(\"ignoreTrailingWhiteSpace\", \"true\")\\\n",
    "                    .load(df_path)\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Street Number'] = [street_num[0] if any(i.isdigit() for i in column) else 'N/A' for street_num in df.Address.apply(lambda s: s.split(\" \",1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 477, in main\n    (\"%d.%d\" % sys.version_info[:2], version))\nException: Python in worker has different version 3.7 than that in driver 3.6, PySpark cannot run with different minor versions. Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-6fef898c27f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'numerical'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_digit_udf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'column'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'column'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'categorical'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mis_digit_udf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'column'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'column'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    438\u001b[0m         \"\"\"\n\u001b[1;32m    439\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m                 \u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
      "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 477, in main\n    (\"%d.%d\" % sys.version_info[:2], version))\nException: Python in worker has different version 3.7 than that in driver 3.6, PySpark cannot run with different minor versions. Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n"
     ]
    }
   ],
   "source": [
    "def is_digit(value):\n",
    "    if value:\n",
    "        return value.isdigit()\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "is_digit_udf = udf(is_digit, BooleanType())\n",
    "\n",
    "df = df.withColumn('numerical', when(is_digit_udf(df['column']), df['column']))\n",
    "df = df.withColumn('categorical', when(~is_digit_udf(df['column']), df['column']))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+-------------+-------+---------------+-----------+--------------------+------+----------+----------+---------+----------+\n",
      "|                 URL|Username|analysis_name|coltype|columnparameter|extracttype|                file|module|operations|      type|   values|   version|\n",
      "+--------------------+--------+-------------+-------+---------------+-----------+--------------------+------+----------+----------+---------+----------+\n",
      "|http://0.0.0.0:80...|  rupesh|   csv_upload|   null|       c2,c4,c5|       null|ElectricProductio...|    ts|  sum,mean|statistics|    c1,c3|Eversion53|\n",
      "|http://0.0.0.0:80...|  rupesh|   csv_upload|    num|             c2|       user|MallCustomers_14.csv|    da|   box_cox|      null|ageboxcox|Mversion23|\n",
      "|http://0.0.0.0:80...|  rupesh|   csv_upload|   null|           null|       null|ElectricProductio...|    da|      null|      null|     null|Eversion53|\n",
      "+--------------------+--------+-------------+-------+---------------+-----------+--------------------+------+----------+----------+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"parsed_url\", explode(split(expr(\"parse_url(URL, 'QUERY')\"), \"&\"))) \\\n",
    "    .withColumn(\"parsed_url\", split(\"parsed_url\", \"=\")) \\\n",
    "    .select(\"URL\",\n",
    "            col(\"parsed_url\").getItem(0).alias(\"param_name\"),\n",
    "            col(\"parsed_url\").getItem(1).alias(\"value\")\n",
    "            ) \\\n",
    "    .groupBy(\"URL\") \\\n",
    "    .pivot(\"param_name\") \\\n",
    "    .agg(first(\"value\")) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "|email|parsed_url|\n",
      "+-----+----------+\n",
      "+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "var=\"email\"\n",
    "df=df.withColumn(\"parsed_url\", explode(split(expr(\"parse_url(\"+var+\", 'SCHEME')\"), \"@\")))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                 URL|          parsed_url|\n",
      "+--------------------+--------------------+\n",
      "|http://0.0.0.0:80...|[analysis_name, c...|\n",
      "|http://0.0.0.0:80...|[file, ElectricPr...|\n",
      "|http://0.0.0.0:80...|[version, Eversio...|\n",
      "|http://0.0.0.0:80...|  [Username, rupesh]|\n",
      "|http://0.0.0.0:80...|        [module, da]|\n",
      "|http://0.0.0.0:80...|[file, ElectricPr...|\n",
      "|http://0.0.0.0:80...|  [type, statistics]|\n",
      "|http://0.0.0.0:80...|  [Username, rupesh]|\n",
      "|http://0.0.0.0:80...|[analysis_name, c...|\n",
      "|http://0.0.0.0:80...|[version, Eversio...|\n",
      "|http://0.0.0.0:80...|        [module, ts]|\n",
      "|http://0.0.0.0:80...|[columnparameter,...|\n",
      "|http://0.0.0.0:80...|[operations, sum,...|\n",
      "|http://0.0.0.0:80...|     [values, c1,c3]|\n",
      "|http://0.0.0.0:80...|        [module, da]|\n",
      "|http://0.0.0.0:80...|[file, MallCustom...|\n",
      "|http://0.0.0.0:80...|  [Username, rupesh]|\n",
      "|http://0.0.0.0:80...|[version, Mversio...|\n",
      "|http://0.0.0.0:80...|[analysis_name, c...|\n",
      "|http://0.0.0.0:80...|      [coltype, num]|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=df.withColumn(\"parsed_url\", split(\"parsed_url\", \"=\"))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+--------------------+\n",
      "|                 URL|     param_name|               value|\n",
      "+--------------------+---------------+--------------------+\n",
      "|http://0.0.0.0:80...|  analysis_name|          csv_upload|\n",
      "|http://0.0.0.0:80...|           file|ElectricProductio...|\n",
      "|http://0.0.0.0:80...|        version|          Eversion53|\n",
      "|http://0.0.0.0:80...|       Username|              rupesh|\n",
      "|http://0.0.0.0:80...|         module|                  da|\n",
      "|http://0.0.0.0:80...|           file|ElectricProductio...|\n",
      "|http://0.0.0.0:80...|           type|          statistics|\n",
      "|http://0.0.0.0:80...|       Username|              rupesh|\n",
      "|http://0.0.0.0:80...|  analysis_name|          csv_upload|\n",
      "|http://0.0.0.0:80...|        version|          Eversion53|\n",
      "|http://0.0.0.0:80...|         module|                  ts|\n",
      "|http://0.0.0.0:80...|columnparameter|            c2,c4,c5|\n",
      "|http://0.0.0.0:80...|     operations|            sum,mean|\n",
      "|http://0.0.0.0:80...|         values|               c1,c3|\n",
      "|http://0.0.0.0:80...|         module|                  da|\n",
      "|http://0.0.0.0:80...|           file|MallCustomers_14.csv|\n",
      "|http://0.0.0.0:80...|       Username|              rupesh|\n",
      "|http://0.0.0.0:80...|        version|          Mversion23|\n",
      "|http://0.0.0.0:80...|  analysis_name|          csv_upload|\n",
      "|http://0.0.0.0:80...|        coltype|                 num|\n",
      "+--------------------+---------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=df.select(\"URL\",col(\"parsed_url\").getItem(0).alias(\"param_name\"),col(\"parsed_url\").getItem(1).alias(\"value\"))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(param_name='analysis_name')\n",
      "Row(param_name='file')\n",
      "Row(param_name='version')\n",
      "Row(param_name='Username')\n",
      "Row(param_name='module')\n",
      "Row(param_name='file')\n",
      "Row(param_name='type')\n",
      "Row(param_name='Username')\n",
      "Row(param_name='analysis_name')\n",
      "Row(param_name='version')\n",
      "Row(param_name='module')\n",
      "Row(param_name='columnparameter')\n",
      "Row(param_name='operations')\n",
      "Row(param_name='values')\n",
      "Row(param_name='module')\n",
      "Row(param_name='file')\n",
      "Row(param_name='Username')\n",
      "Row(param_name='version')\n",
      "Row(param_name='analysis_name')\n",
      "Row(param_name='coltype')\n",
      "Row(param_name='columnparameter')\n",
      "Row(param_name='extracttype')\n",
      "Row(param_name='operations')\n",
      "Row(param_name='values')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-117-3cb955465222>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_replace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'URL'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'param_name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    438\u001b[0m         \"\"\"\n\u001b[1;32m    439\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1301\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1303\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1031\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1033\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1034\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1199\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1200\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1201\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in df.select(\"param_name\").collect():\n",
    "    print(i)\n",
    "    df= df.na.replace(to_replace=i[0],value='URL'+'_'+i[0],subset=['param_name'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.groupBy(\"URL\").pivot(\"param_name\").agg(first(\"value\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['URL',\n",
       " 'Username',\n",
       " 'analysis_name',\n",
       " 'coltype',\n",
       " 'columnparameter',\n",
       " 'extracttype',\n",
       " 'file',\n",
       " 'module',\n",
       " 'operations',\n",
       " 'type',\n",
       " 'values',\n",
       " 'version']"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "s=[\"URL\"+'_'+col_name for col_name in df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.select([col(c).alias(\"URL\"+\"_\" + c) for c in df.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['URL_URL',\n",
       " 'URL_Username',\n",
       " 'URL_analysis_name',\n",
       " 'URL_coltype',\n",
       " 'URL_columnparameter',\n",
       " 'URL_extracttype',\n",
       " 'URL_file',\n",
       " 'URL_module',\n",
       " 'URL_operations',\n",
       " 'URL_type',\n",
       " 'URL_values',\n",
       " 'URL_version']"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+-----------------+-----------+-------------------+---------------+--------------------+----------+--------------+----------+----------+-----------+\n",
      "|                 URL|URL_Username|URL_analysis_name|URL_coltype|URL_columnparameter|URL_extracttype|            URL_file|URL_module|URL_operations|  URL_type|URL_values|URL_version|\n",
      "+--------------------+------------+-----------------+-----------+-------------------+---------------+--------------------+----------+--------------+----------+----------+-----------+\n",
      "|http://0.0.0.0:80...|      rupesh|       csv_upload|       null|           c2,c4,c5|           null|ElectricProductio...|        ts|      sum,mean|statistics|     c1,c3| Eversion53|\n",
      "|http://0.0.0.0:80...|      rupesh|       csv_upload|        num|                 c2|           user|MallCustomers_14.csv|        da|       box_cox|      null| ageboxcox| Mversion23|\n",
      "|http://0.0.0.0:80...|      rupesh|       csv_upload|       null|               null|           null|ElectricProductio...|        da|          null|      null|      null| Eversion53|\n",
      "+--------------------+------------+-----------------+-----------+-------------------+---------------+--------------------+----------+--------------+----------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumnRenamed('URL_URL', 'URL')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HTTP_to_column(df,url_column):\n",
    "    d=df\n",
    "    df=df.withColumn(\"parsed_url\", explode(split(expr(\"parse_url(\"+url_column+\", 'QUERY')\"), \"&\"))) \\\n",
    "    .withColumn(\"parsed_url\", split(\"parsed_url\", \"=\")) \\\n",
    "    .select(url_column ,col(\"parsed_url\").getItem(0).alias(\"param_name\"),col(\"parsed_url\").getItem(1).alias(\"value\")) \\\n",
    "    .groupBy(url_column).pivot(\"param_name\").agg(first(\"value\"))\n",
    "    df=df.select([col(c).alias(url_column+\"_\" + c) for c in df.columns])\n",
    "    df = df.withColumnRenamed(url_column+\"_\"+url_column, url_column)\n",
    "    d=d.join(df, url_column)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                 URL|count|\n",
      "+--------------------+-----+\n",
      "|http://0.0.0.0:80...|   56|\n",
      "|http://0.0.0.0:80...|   76|\n",
      "|http://0.0.0.0:80...|   89|\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_path =\"/home/rupeshr/Desktop/TSA_Python/sample_url.csv\"\n",
    "df = spark.read.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(df_path)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('URL', 'string'), ('count', 'int')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+-----------------+-----------+-------------------+---------------+--------------------+----------+--------------+----------+----------+-----------+\n",
      "|                 URL|URL_Username|URL_analysis_name|URL_coltype|URL_columnparameter|URL_extracttype|            URL_file|URL_module|URL_operations|  URL_type|URL_values|URL_version|\n",
      "+--------------------+------------+-----------------+-----------+-------------------+---------------+--------------------+----------+--------------+----------+----------+-----------+\n",
      "|http://0.0.0.0:80...|      rupesh|       csv_upload|       null|           c2,c4,c5|           null|ElectricProductio...|        ts|      sum,mean|statistics|     c1,c3| Eversion53|\n",
      "|http://0.0.0.0:80...|      rupesh|       csv_upload|        num|                 c2|           user|MallCustomers_14.csv|        da|       box_cox|      null| ageboxcox| Mversion23|\n",
      "|http://0.0.0.0:80...|      rupesh|       csv_upload|       null|               null|           null|ElectricProductio...|        da|          null|      null|      null| Eversion53|\n",
      "+--------------------+------------+-----------------+-----------+-------------------+---------------+--------------------+----------+--------------+----------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s=HTTP_to_column(df,\"URL\")\n",
    "s.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+------------+-----------------+-----------+-------------------+---------------+--------------------+----------+--------------+----------+----------+-----------+\n",
      "|                 URL|count|URL_Username|URL_analysis_name|URL_coltype|URL_columnparameter|URL_extracttype|            URL_file|URL_module|URL_operations|  URL_type|URL_values|URL_version|\n",
      "+--------------------+-----+------------+-----------------+-----------+-------------------+---------------+--------------------+----------+--------------+----------+----------+-----------+\n",
      "|http://0.0.0.0:80...|   76|      rupesh|       csv_upload|       null|           c2,c4,c5|           null|ElectricProductio...|        ts|      sum,mean|statistics|     c1,c3| Eversion53|\n",
      "|http://0.0.0.0:80...|   89|      rupesh|       csv_upload|        num|                 c2|           user|MallCustomers_14.csv|        da|       box_cox|      null| ageboxcox| Mversion23|\n",
      "|http://0.0.0.0:80...|   56|      rupesh|       csv_upload|       null|               null|           null|ElectricProductio...|        da|          null|      null|      null| Eversion53|\n",
      "+--------------------+-----+------------+-----------------+-----------+-------------------+---------------+--------------------+----------+--------------+----------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=df.join(s, 'URL')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+---------+----------+--------+----------+----------+----------+----------+-------------+----------+-----------+--------+----------+--------+--------+\n",
      "|                 URL|URL_amount|URL_cost2|URL_member|URL_name|URL_param1|URL_param2|URL_param3|URL_param4|URL_parameter|URL_params|URL_product|URL_rent|URL_source|URL_test|URL_user|\n",
      "+--------------------+----------+---------+----------+--------+----------+----------+----------+----------+-------------+----------+-----------+--------+----------+--------+--------+\n",
      "|https://www.examp...|      null|     null|      null|     api|      null|      null|      null|      null|         null|      null|       null|    null|      null|    null|    test|\n",
      "|https://www.examp...|      null|     null|      null|    user|      null|      null|      null|      null|         null|      null|          3|    null|      null|    user|    null|\n",
      "|https://www.examp...|      null|     null|      null|    null|         2|         b|      null|      null|         null|      null|       null|    null|      null|    null|    null|\n",
      "|https://www.examp...|     50000|     null|         5|    null|      null|      null|      null|      null|         null|      null|       null|    1000|      null|    null|    null|\n",
      "|https://www.examp...|      null|      500|      null|    null|      null|      null|      null|      null|         null|      null|          2|    null|         4|    null|    null|\n",
      "|https://www.examp...|      null|     null|      null|    null|      null|      null|      null|      null|          key|     value|       null|    null|      null|    null|    null|\n",
      "|https://www.examp...|      null|     null|      null|    null|      null|      null|         2|         5|         null|      null|       null|    null|      null|    null|    null|\n",
      "|https://www.examp...|      null|     null|      null|    null|         1|         a|      null|      null|         null|      null|       null|    null|      null|    null|    null|\n",
      "|https://www.examp...|      null|     null|      null|    null|         2|         5|      null|      null|         null|      null|         15|    null|      null|    null|    null|\n",
      "|https://www.examp...|      null|     null|      null|    null|         3|         c|      null|      null|         null|      null|       null|    null|      null|    null|    null|\n",
      "|https://www.examp...|      null|     null|      null|    null|         3|        c3|      null|      null|         null|      null|       null|    null|      null|    null|    null|\n",
      "|https://www.examp...|      null|       c3|      null|    null|      null|      null|      null|      null|         null|      null|         34|    null|        65|    null|    null|\n",
      "|https://www.examp...|      null|        c|      null|    null|      null|      null|      null|      null|         null|      null|          6|    null|         3|    null|    null|\n",
      "+--------------------+----------+---------+----------+--------+----------+----------+----------+----------+-------------+----------+-----------+--------+----------+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "split_email = F.split(df.email, \"@\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column<b'split(email, @, -1)'>\n"
     ]
    }
   ],
   "source": [
    "print(split_email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               email|\n",
      "+--------------------+\n",
      "|[rupeshrupeesgmai...|\n",
      "| [kokojambo, mon.eu]|\n",
      "|      [lol, fsa.org]|\n",
      "|      [ii, koko.com]|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=df.withColumn(\"email\", split(\"email\", \"@\"))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_email(df, email_column):\n",
    "    df=df.select(email_column,split(email_column,\"@\").getItem(0).alias(\"username\"),split(email_column,\"@\").getItem(1).alias(\"domain\"))\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               email|\n",
      "+--------------------+\n",
      "|rupeshrupees@gmai...|\n",
      "|    kokojambo@mon.eu|\n",
      "|         lol@fsa.org|\n",
      "|        ii@koko.com |\n",
      "|rupesh.subramani@...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_path =\"/home/rupeshr/Desktop/TSA_Python/sample_email.csv\"\n",
    "df = spark.read.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(df_path)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------+---------+\n",
      "|               email|        username|   domain|\n",
      "+--------------------+----------------+---------+\n",
      "|rupeshrupees@gmai...|    rupeshrupees|gmail.com|\n",
      "|    kokojambo@mon.eu|       kokojambo|   mon.eu|\n",
      "|         lol@fsa.org|             lol|  fsa.org|\n",
      "|        ii@koko.com |              ii|koko.com |\n",
      "|rupesh.subramani@...|rupesh.subramani|gmail.com|\n",
      "+--------------------+----------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s=split_email(df, \"email\")\n",
    "s.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------+---------+\n",
      "|               email|        username|   domain|\n",
      "+--------------------+----------------+---------+\n",
      "|rupeshrupees@gmai...|    rupeshrupees|gmail.com|\n",
      "|    kokojambo@mon.eu|       kokojambo|   mon.eu|\n",
      "|         lol@fsa.org|             lol|  fsa.org|\n",
      "|        ii@koko.com |              ii|koko.com |\n",
      "|rupesh.subramani@...|rupesh.subramani|gmail.com|\n",
      "+--------------------+----------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=df.join(s, 'email')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import *\n",
    "from pyspark.sql.functions import UserDefinedFunction, udf\n",
    "from pyspark.sql.functions import lit\n",
    "import ast\n",
    "import numpy as np\n",
    "import traceback\n",
    "from math import ceil,floor\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+---+------------------+----------------------+\n",
      "|CustomerID| Genre|Age|Annual Income (k$)|Spending Score (1-100)|\n",
      "+----------+------+---+------------------+----------------------+\n",
      "|         1|  Male| 19|                15|                    39|\n",
      "|         2|  Male| 21|                15|                    81|\n",
      "|         3|Female| 20|                16|                     6|\n",
      "|         4|Female| 23|                16|                    77|\n",
      "|         5|Female| 31|                17|                    40|\n",
      "|         6|Female| 22|                17|                    76|\n",
      "|         7|Female| 35|                18|                     6|\n",
      "|         8|Female| 23|                18|                    94|\n",
      "|         9|  Male| 64|                19|                     3|\n",
      "|        10|Female| 30|                19|                    72|\n",
      "|        11|  Male| 67|                19|                    14|\n",
      "|        12|Female| 35|                19|                    99|\n",
      "|        13|Female| 58|                20|                    15|\n",
      "|        14|Female| 24|                20|                    77|\n",
      "|        15|  Male| 37|                20|                    13|\n",
      "|        16|  Male| 22|                20|                    79|\n",
      "|        17|Female| 35|                21|                    35|\n",
      "|        18|  Male| 20|                21|                    66|\n",
      "|        19|  Male| 52|                23|                    29|\n",
      "|        20|Female| 35|                23|                    98|\n",
      "+----------+------+---+------------------+----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_path =\"/home/rupeshr/Downloads/Mall_Customers.csv\"\n",
    "df = spark.read.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(df_path)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[70] [18]\n"
     ]
    }
   ],
   "source": [
    "val = df.groupby().max('Age').collect()[0]\n",
    "vals = df.groupby().min('Age').collect()[0]\n",
    "c = [val[0]] # max\n",
    "d = [vals[0]] # min\n",
    "print(c, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 18, 1: 25, 2: 30, 3: 40, 4: 50, 5: 70}\n"
     ]
    }
   ],
   "source": [
    "splits=d + [25,30,40,50] + c\n",
    "splits_dict = {i:splits[i] for i in range(len(splits))}\n",
    "print(splits_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=[10,20,30,40,50,60,70]\n",
    "bucketizer = Bucketizer(splits=x, inputCol=\"Age\",outputCol=\"age_result\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+---+------------------+----------------------+----------+\n",
      "|CustomerID| Genre|Age|Annual Income (k$)|Spending Score (1-100)|age_result|\n",
      "+----------+------+---+------------------+----------------------+----------+\n",
      "|         1|  Male| 19|                15|                    39|       0.0|\n",
      "|         2|  Male| 21|                15|                    81|       1.0|\n",
      "|         3|Female| 20|                16|                     6|       1.0|\n",
      "|         4|Female| 23|                16|                    77|       1.0|\n",
      "|         5|Female| 31|                17|                    40|       2.0|\n",
      "|         6|Female| 22|                17|                    76|       1.0|\n",
      "|         7|Female| 35|                18|                     6|       2.0|\n",
      "|         8|Female| 23|                18|                    94|       1.0|\n",
      "|         9|  Male| 64|                19|                     3|       5.0|\n",
      "|        10|Female| 30|                19|                    72|       2.0|\n",
      "|        11|  Male| 67|                19|                    14|       5.0|\n",
      "|        12|Female| 35|                19|                    99|       2.0|\n",
      "|        13|Female| 58|                20|                    15|       4.0|\n",
      "|        14|Female| 24|                20|                    77|       1.0|\n",
      "|        15|  Male| 37|                20|                    13|       2.0|\n",
      "|        16|  Male| 22|                20|                    79|       1.0|\n",
      "|        17|Female| 35|                21|                    35|       2.0|\n",
      "|        18|  Male| 20|                21|                    66|       1.0|\n",
      "|        19|  Male| 52|                23|                    29|       4.0|\n",
      "|        20|Female| 35|                23|                    98|       2.0|\n",
      "+----------+------+---+------------------+----------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bucketed = bucketizer.transform(df)\n",
    "bucketed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+---+------------------+----------------------+----------+\n",
      "|CustomerID| Genre|Age|Annual Income (k$)|Spending Score (1-100)|age_result|\n",
      "+----------+------+---+------------------+----------------------+----------+\n",
      "|         1|  Male| 19|                15|                    39|      18.0|\n",
      "|         2|  Male| 21|                15|                    81|      25.0|\n",
      "|         3|Female| 20|                16|                     6|      25.0|\n",
      "|         4|Female| 23|                16|                    77|      25.0|\n",
      "|         5|Female| 31|                17|                    40|      30.0|\n",
      "|         6|Female| 22|                17|                    76|      25.0|\n",
      "|         7|Female| 35|                18|                     6|      30.0|\n",
      "|         8|Female| 23|                18|                    94|      25.0|\n",
      "|         9|  Male| 64|                19|                     3|      70.0|\n",
      "|        10|Female| 30|                19|                    72|      30.0|\n",
      "|        11|  Male| 67|                19|                    14|      70.0|\n",
      "|        12|Female| 35|                19|                    99|      30.0|\n",
      "|        13|Female| 58|                20|                    15|      50.0|\n",
      "|        14|Female| 24|                20|                    77|      25.0|\n",
      "|        15|  Male| 37|                20|                    13|      30.0|\n",
      "|        16|  Male| 22|                20|                    79|      25.0|\n",
      "|        17|Female| 35|                21|                    35|      30.0|\n",
      "|        18|  Male| 20|                21|                    66|      25.0|\n",
      "|        19|  Male| 52|                23|                    29|      50.0|\n",
      "|        20|Female| 35|                23|                    98|      30.0|\n",
      "+----------+------+---+------------------+----------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bucketed = bucketed.replace(to_replace=splits_dict, subset=['age_result'])\n",
    "bucketed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = []\n",
    "for i in range(len(x[:-1])):\n",
    "    q = (np.round(x[i],6), np.round(x[i+1],6))\n",
    "    s.append(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(10, 20), (20, 30), (30, 40), (40, 50), (50, 60), (60, 70)]\n"
     ]
    }
   ],
   "source": [
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testUDF(col, s):\n",
    "    if col:\n",
    "        s = ast.literal_eval(str(s))\n",
    "        # s = [(17, 35), (35, 44), (44, 92)]\n",
    "        return str(s[int(col)])\n",
    "    else:\n",
    "        return\n",
    "            # myudf = UserDefinedFunction(testUDF, StringType())\n",
    "myudf = udf(testUDF, StringType())\n",
    "f = bucketed.withColumn('age_source', myudf('age_result', lit(str(s))))\n",
    "sd = f.drop(\"age_result\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[CustomerID: int, Genre: string, Age: int, Annual Income (k$): int, Spending Score (1-100): int, age_source: string]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
