{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "SPARK_HOME=\"/opt/spark\"\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"python3\"\n",
    "# os.environ[\"PYSPARK_PYTHON\"]=\"python\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"]=\"jupyter\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON_OPTS\"]=\"notebook\"\n",
    "os.environ[\"SPARK_HOME\"] = SPARK_HOME\n",
    "os.environ[\"SPARK_CLASSPATH\"] = \"/opt/spark/jars/sqljdbc4.jar\"\n",
    "\n",
    " \n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"Task_3\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pyspark.sql import SQLContext\n",
    "# sqlContext = SQLContext(spark)\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- DATE: timestamp (nullable = true)\n",
      " |-- IPG2211A2N: double (nullable = true)\n",
      "\n",
      "+---+-------------------+----------+\n",
      "|_c0|               DATE|IPG2211A2N|\n",
      "+---+-------------------+----------+\n",
      "|  0|1985-01-01 00:00:00|   72.5052|\n",
      "|  1|1985-02-01 00:00:00|    70.672|\n",
      "|  2|1985-03-01 00:00:00|   62.4502|\n",
      "|  3|1985-04-01 00:00:00|   57.4714|\n",
      "|  4|1985-05-01 00:00:00|   55.3151|\n",
      "|  5|1985-06-01 00:00:00|   58.0904|\n",
      "|  6|1985-07-01 00:00:00|   62.6202|\n",
      "|  7|1985-08-01 00:00:00|   63.2485|\n",
      "|  8|1985-09-01 00:00:00|   60.5846|\n",
      "|  9|1985-10-01 00:00:00|   56.3154|\n",
      "| 10|1985-11-01 00:00:00|   58.0005|\n",
      "| 11|1985-12-01 00:00:00|   68.7145|\n",
      "| 12|1986-01-01 00:00:00|   73.3057|\n",
      "| 13|1986-02-01 00:00:00|   67.9869|\n",
      "| 14|1986-03-01 00:00:00|   62.2221|\n",
      "| 15|1986-04-01 00:00:00|   57.0329|\n",
      "| 16|1986-05-01 00:00:00|   55.8137|\n",
      "| 17|1986-06-01 00:00:00|   59.9005|\n",
      "| 18|1986-07-01 00:00:00|   65.7655|\n",
      "| 19|1986-08-01 00:00:00|   64.4816|\n",
      "| 20|1986-09-01 00:00:00|   61.0005|\n",
      "| 21|1986-10-01 00:00:00|   57.5322|\n",
      "| 22|1986-11-01 00:00:00|   59.3417|\n",
      "| 23|1986-12-01 00:00:00|   68.1354|\n",
      "| 24|1987-01-01 00:00:00|   73.8152|\n",
      "| 25|1987-02-01 00:00:00|    70.062|\n",
      "| 26|1987-03-01 00:00:00|     65.61|\n",
      "| 27|1987-04-01 00:00:00|   60.1586|\n",
      "| 28|1987-05-01 00:00:00|   58.8734|\n",
      "| 29|1987-06-01 00:00:00|   63.8918|\n",
      "+---+-------------------+----------+\n",
      "only showing top 30 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_path = \"/home/rupeshr/Desktop/TSA_Python/dataset/time series dataset/archive/Electric_Production1.csv\"\n",
    "df =spark.read.format(\"com.databricks.spark.csv\")\\\n",
    "                    .option(\"multiline\", \"true\")\\\n",
    "                    .option(\"header\", \"true\")\\\n",
    "                    .option(\"inferSchema\", \"true\")\\\n",
    "                    .option(\"sep\", ',')\\\n",
    "                    .option('escape', '\\\"')\\\n",
    "                    .option(\"allowSingleQuotes\", \"true\")\\\n",
    "                    .option(\"ignoreLeadingWhiteSpace\", \"true\")\\\n",
    "                    .option(\"ignoreTrailingWhiteSpace\", \"true\")\\\n",
    "                    .load(df_path)\n",
    "df.printSchema()\n",
    "df.show(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'start': [Row(min(DATE)=datetime.datetime(1985, 1, 1, 0, 0))], 'end': [Row(max(DATE)=datetime.datetime(2018, 1, 1, 0, 0))]}\n"
     ]
    }
   ],
   "source": [
    "def extreme_date(df,datecolumn):\n",
    "    start=df.select(min(datecolumn)).collect()\n",
    "    end=df.select(max(datecolumn)).collect()\n",
    "    x={'start':start, 'end':end}\n",
    "    return x\n",
    "x=extreme_date(df,'DATE')\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+----------+\n",
      "|_c0|               DATE|IPG2211A2N|\n",
      "+---+-------------------+----------+\n",
      "| 12|1986-01-01 00:00:00|   73.3057|\n",
      "| 13|1986-02-01 00:00:00|   67.9869|\n",
      "| 14|1986-03-01 00:00:00|   62.2221|\n",
      "| 15|1986-04-01 00:00:00|   57.0329|\n",
      "| 16|1986-05-01 00:00:00|   55.8137|\n",
      "| 17|1986-06-01 00:00:00|   59.9005|\n",
      "| 18|1986-07-01 00:00:00|   65.7655|\n",
      "| 19|1986-08-01 00:00:00|   64.4816|\n",
      "| 20|1986-09-01 00:00:00|   61.0005|\n",
      "| 21|1986-10-01 00:00:00|   57.5322|\n",
      "+---+-------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def filter_dataslicing(df,datecolumn,start,end):\n",
    "    df=df.filter((df[datecolumn] >= start) & (df[datecolumn] <= end))\n",
    "    return df\n",
    "df1=filter_dataslicing(df,datecolumn='DATE',start='1986-01-01 00:00:00',end='1986-10-01 00:00:00')\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_c0</th>\n",
       "      <th>DATE</th>\n",
       "      <th>IPG2211A2N</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1985-01-01</td>\n",
       "      <td>72.5052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1985-02-01</td>\n",
       "      <td>70.6720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1985-03-01</td>\n",
       "      <td>62.4502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1985-04-01</td>\n",
       "      <td>57.4714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1985-05-01</td>\n",
       "      <td>55.3151</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   _c0       DATE  IPG2211A2N\n",
       "0    0 1985-01-01     72.5052\n",
       "1    1 1985-02-01     70.6720\n",
       "2    2 1985-03-01     62.4502\n",
       "3    3 1985-04-01     57.4714\n",
       "4    4 1985-05-01     55.3151"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=df.toPandas()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+----------+\n",
      "|_c0|               DATE|IPG2211A2N|\n",
      "+---+-------------------+----------+\n",
      "|  0|1985-01-01 00:00:00|   72.5052|\n",
      "|  1|1985-02-01 00:00:00|    70.672|\n",
      "|  2|1985-03-01 00:00:00|   62.4502|\n",
      "|  3|1985-04-01 00:00:00|   57.4714|\n",
      "|  4|1985-05-01 00:00:00|   55.3151|\n",
      "|  5|1985-06-01 00:00:00|   58.0904|\n",
      "|  6|1985-07-01 00:00:00|   62.6202|\n",
      "|  7|1985-08-01 00:00:00|   63.2485|\n",
      "|  8|1985-09-01 00:00:00|   60.5846|\n",
      "|  9|1985-10-01 00:00:00|   56.3154|\n",
      "| 10|1985-11-01 00:00:00|   58.0005|\n",
      "| 11|1985-12-01 00:00:00|   68.7145|\n",
      "| 12|1986-01-01 00:00:00|   73.3057|\n",
      "| 13|1986-02-01 00:00:00|   67.9869|\n",
      "| 14|1986-03-01 00:00:00|   62.2221|\n",
      "| 15|1986-04-01 00:00:00|   57.0329|\n",
      "| 16|1986-05-01 00:00:00|   55.8137|\n",
      "| 17|1986-06-01 00:00:00|   59.9005|\n",
      "| 18|1986-07-01 00:00:00|   65.7655|\n",
      "| 19|1986-08-01 00:00:00|   64.4816|\n",
      "+---+-------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = spark.createDataFrame(data)\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extreme_date(df,datecolumn):\n",
    "    start=df.select(min(datecolumn)).collect()\n",
    "    end=df.select(max(datecolumn)).collect()\n",
    "    x={'start':start, 'end':end}\n",
    "    return x\n",
    "\n",
    "class timeseries_extreme_date(APIView):\n",
    "    def post(self,request):\n",
    "        #auth_keyword, token = get_authorization_header(request).split()\n",
    "        #token = token.decode(\"utf-8\")\n",
    "        #check = headercheck(token)\n",
    "        check ='ok'\n",
    "        if check == 'ok':\n",
    "            pass\n",
    "        else:\n",
    "            return Response({\"status\": \"validation\", \"error\": \"user has been killed\"})\n",
    "        data = request.data\n",
    "        path=data['path']\n",
    "        date=data['date']\n",
    "        try:\n",
    "            import numpy as np\n",
    "            #file = match_version(version, analysis, filename)\n",
    "            #data = read_file(file)  # spark data frame\n",
    "            #if str(type(data)) == \"<class 'str'>\":\n",
    "             #   data = read_file(filename)\n",
    "            #data = data.toPandas()\n",
    "            df=pd.read_csv(path)\n",
    "            df[date] = pd.to_datetime(df[date])\n",
    "            df = df.sort_values(by=date)\n",
    "            df = spark.createDataFrame(df)\n",
    "            #df.set_index(date, inplace=True)\n",
    "            output=extreme_date(df,date)\n",
    "            print(output)\n",
    "            return Response({'status':'success'})\n",
    "        except Exception as e:\n",
    "            traceback.print_exc()\n",
    "            return Response({\"status\": \"failure\",\"error\":\"Please refer the logs in Logs->Con"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_dataslicing(df,datecolumn,start,end):\n",
    "    df=df.filter((df[datecolumn] >= start) & (df[datecolumn] <= end))\n",
    "    return df\n",
    "\n",
    "class timeseries_dataslicing(APIView):\n",
    "    def post(self,request):\n",
    "        #auth_keyword, token = get_authorization_header(request).split()\n",
    "        #token = token.decode(\"utf-8\")\n",
    "        #check = headercheck(token)\n",
    "        check ='ok'\n",
    "        if check == 'ok':\n",
    "            pass\n",
    "        else:\n",
    "            return Response({\"status\": \"validation\", \"error\": \"user has been killed\"})\n",
    "        data = request.data\n",
    "        path=data['path']\n",
    "        date=data['date']\n",
    "        start=data['startdate']\n",
    "        end=data['enddate']\n",
    "        try:\n",
    "            import numpy as np\n",
    "            #file = match_version(version, analysis, filename)\n",
    "            #data = read_file(file)  # spark data frame\n",
    "            #if str(type(data)) == \"<class 'str'>\":\n",
    "             #   data = read_file(filename)\n",
    "            #data = data.toPandas()\n",
    "            df=pd.read_csv(path)\n",
    "            df[date] = pd.to_datetime(df[date])\n",
    "            df = df.sort_values(by=date)\n",
    "            df = spark.createDataFrame(df)\n",
    "            df=filter_dataslicing(df,date,start,end)\n",
    "            print(df)\n",
    "            return Response({'status':'success'})\n",
    "        except Exception as e:\n",
    "            traceback.print_exc()\n",
    "            return Response({\"status\": \"failure\",\"error\":\"Please refer the logs in Logs->Container Log\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
