{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "SPARK_HOME=\"/opt/spark\"\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"python3\"\n",
    "# os.environ[\"PYSPARK_PYTHON\"]=\"python\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"]=\"jupyter\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON_OPTS\"]=\"notebook\"\n",
    "os.environ[\"SPARK_HOME\"] = SPARK_HOME\n",
    "os.environ[\"SPARK_CLASSPATH\"] = \"/opt/spark/jars/sqljdbc4.jar\"\n",
    "\n",
    " \n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"Task_3\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pyspark.sql import SQLContext\n",
    "# sqlContext = SQLContext(spark)\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- count: integer (nullable = true)\n",
      "\n",
      "+----------+--------+-----+\n",
      "|      Date|category|count|\n",
      "+----------+--------+-----+\n",
      "|1996-01-01|   shade|   23|\n",
      "|1996-01-02|     sun|   21|\n",
      "|1996-01-03|     sun|   61|\n",
      "|1996-01-04|   shade| null|\n",
      "|1996-01-05|    null| null|\n",
      "|1996-01-06|    null| null|\n",
      "|1996-01-07|    null| null|\n",
      "|1996-01-08|    null| null|\n",
      "|1996-01-09|     sun|   14|\n",
      "|1996-01-10|   shade|   17|\n",
      "|1996-01-11|   shade|   12|\n",
      "|1996-01-12|     sun|    3|\n",
      "|1996-01-13|   shade|   11|\n",
      "|1996-01-14|   shade|   11|\n",
      "|1996-01-15|     sun|   23|\n",
      "+----------+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_path = \"/home/rupeshr/Desktop/TSA_Python/splitting_dataset/categorydataset.csv\"\n",
    "df =spark.read.format(\"com.databricks.spark.csv\")\\\n",
    "                    .option(\"multiline\", \"true\")\\\n",
    "                    .option(\"header\", \"true\")\\\n",
    "                    .option(\"inferSchema\", \"true\")\\\n",
    "                    .option(\"sep\", ',')\\\n",
    "                    .option('escape', '\\\"')\\\n",
    "                    .option(\"allowSingleQuotes\", \"true\")\\\n",
    "                    .option(\"ignoreLeadingWhiteSpace\", \"true\")\\\n",
    "                    .option(\"ignoreTrailingWhiteSpace\", \"true\")\\\n",
    "                    .load(df_path)\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function code\n",
    "\n",
    "import sys\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import last,first\n",
    "from rest_framework.response import Response\n",
    "\n",
    "\n",
    "def fillemptycells(*,df,column,method,Groupby=None, Orderby=None):\n",
    "    try:\n",
    "        for i in df.columns:\n",
    "            if i in column:\n",
    "                s = df.select(i).dtypes[0]\n",
    "                if s[1] == \"int\" or s[1] == \"float\" or s[1] == \"double\" or s[1] == \"bigint\" or s[1] == \"long\" or s[1] == \"string\":\n",
    "        \n",
    "                    if Groupby ==None and Orderby ==None:\n",
    "                        \n",
    "                        if method =='ffill':\n",
    "                            window = Window.rowsBetween(-sys.maxsize, 0)\n",
    "                            filled_column = last(df[column], ignorenulls=True).over(window)\n",
    "                            spark_df_filled =df.withColumn('renamed_value', filled_column).drop('index',column)\n",
    "                            spark_df_filled=spark_df_filled.withColumnRenamed('renamed_value', column)\n",
    "                        \n",
    "                    \n",
    "                        elif method =='bfill':\n",
    "                            window = Window.rowsBetween(0,sys.maxsize)\n",
    "                            filled_column = first(df[column], ignorenulls=True).over(window)\n",
    "                            spark_df_filled =df.withColumn('renamed_value', filled_column).drop('index',column)\n",
    "                            spark_df_filled=spark_df_filled.withColumnRenamed('renamed_value', column)\n",
    "                \n",
    "                    elif Groupby is not None and Orderby is not None:\n",
    "                \n",
    "                        if method =='ffill':\n",
    "                            window = Window.partitionBy(df[Groupby]).orderBy(df[Orderby]).rowsBetween(-sys.maxsize, 0)\n",
    "                            filled_column = last(df[column], ignorenulls=True).over(window)\n",
    "                            spark_df_filled =df.withColumn('renamed_value', filled_column).drop('index',column)\n",
    "                            spark_df_filled=spark_df_filled.withColumnRenamed('renamed_value', column)\n",
    "                    \n",
    "                        elif method =='bfill':\n",
    "                            window = Window.partitionBy(df[Groupby]).orderBy(df[Orderby]).rowsBetween(0,sys.maxsize)\n",
    "                            filled_column = last(df[column], ignorenulls=True).over(window)\n",
    "                            spark_df_filled =df.withColumn('renamed_value', filled_column).drop('index',column)\n",
    "                            spark_df_filled=spark_df_filled.withColumnRenamed('renamed_value', column)\n",
    "                \n",
    "        return spark_df_filled\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return Response({\"status\": \"failure\", \"error\": str(e)})\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+-----+\n",
      "|      Date|category|count|\n",
      "+----------+--------+-----+\n",
      "|1996-01-01|   shade|   23|\n",
      "|1996-01-02|     sun|   21|\n",
      "|1996-01-03|     sun|   61|\n",
      "|1996-01-04|   shade| null|\n",
      "|1996-01-05|   shade| null|\n",
      "|1996-01-06|   shade| null|\n",
      "|1996-01-07|   shade| null|\n",
      "|1996-01-08|   shade| null|\n",
      "|1996-01-09|     sun|   14|\n",
      "|1996-01-10|   shade|   17|\n",
      "|1996-01-11|   shade|   12|\n",
      "|1996-01-12|     sun|    3|\n",
      "|1996-01-13|   shade|   11|\n",
      "|1996-01-14|   shade|   11|\n",
      "|1996-01-15|     sun|   23|\n",
      "+----------+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_path = \"/home/rupeshr/Desktop/TSA_Python/splitting_dataset/categorydataset.csv\"\n",
    "df = spark.read.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(df_path)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+-----+\n",
      "|      Date|category|count|\n",
      "+----------+--------+-----+\n",
      "|1996-01-02|     sun|   21|\n",
      "|1996-01-03|     sun|   61|\n",
      "|1996-01-09|     sun|   14|\n",
      "|1996-01-12|     sun|    3|\n",
      "|1996-01-15|     sun|   23|\n",
      "|1996-01-01|   shade|   23|\n",
      "|1996-01-04|   shade|   23|\n",
      "|1996-01-05|   shade|   23|\n",
      "|1996-01-06|   shade|   23|\n",
      "|1996-01-07|   shade|   23|\n",
      "|1996-01-08|   shade|   23|\n",
      "|1996-01-10|   shade|   17|\n",
      "|1996-01-11|   shade|   12|\n",
      "|1996-01-13|   shade|   11|\n",
      "|1996-01-14|   shade|   11|\n",
      "+----------+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1=fillemptycells(df=df,column='count',method='ffill',Groupby='category', Orderby='Date')\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class API code\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import last, first\n",
    "from rest_framework.response import Response\n",
    "\n",
    "\n",
    "def fillemptycells(*, df, column, method, Groupby=None, Orderby=None):\n",
    "    try:\n",
    "        for i in df.columns:\n",
    "            if i in column:\n",
    "                s = df.select(i).dtypes[0]\n",
    "                if s[1] == \"int\" or s[1] == \"float\" or s[1] == \"double\" or s[1] == \"bigint\" or s[1] == \"long\" or s[1] == \"string\":\n",
    "                    try:\n",
    "                        if Groupby == None and Orderby == None:\n",
    "                            try:\n",
    "                                if method == 'ffill':\n",
    "                                    window = Window.rowsBetween(-sys.maxsize, 0)\n",
    "                                    filled_column = last(df[column], ignorenulls=True).over(window)\n",
    "                                    spark_df_filled = df.withColumn('renamed_value', filled_column).drop('index', column)\n",
    "                                    spark_df_filled = spark_df_filled.withColumnRenamed('renamed_value', column)\n",
    "\n",
    "\n",
    "                                elif method == 'bfill':\n",
    "                                    window = Window.rowsBetween(0, sys.maxsize)\n",
    "                                    filled_column = first(df[column], ignorenulls=True).over(window)\n",
    "                                    spark_df_filled = df.withColumn('renamed_value', filled_column).drop('index', column)\n",
    "                                    spark_df_filled = spark_df_filled.withColumnRenamed('renamed_value', column)\n",
    "                            except:\n",
    "                                return Response({\"status\": \"validation\", \"error\": \"select one column alone to change null into filled values\"})\n",
    "\n",
    "                        elif Groupby is not None and Orderby is not None:\n",
    "                            try:\n",
    "                                if method == 'ffill':\n",
    "                                    window = Window.partitionBy(df[Groupby]).orderBy(df[Orderby]).rowsBetween(-sys.maxsize, 0)\n",
    "                                    filled_column = last(df[column], ignorenulls=True).over(window)\n",
    "                                    spark_df_filled = df.withColumn('renamed_value', filled_column).drop('index', column)\n",
    "                                    spark_df_filled = spark_df_filled.withColumnRenamed('renamed_value', column)\n",
    "\n",
    "                                elif method == 'bfill':\n",
    "                                    window = Window.partitionBy(df[Groupby]).orderBy(df[Orderby]).rowsBetween(0, sys.maxsize)\n",
    "                                    filled_column = last(df[column], ignorenulls=True).over(window)\n",
    "                                    spark_df_filled = df.withColumn('renamed_value', filled_column).drop('index', column)\n",
    "                                    spark_df_filled = spark_df_filled.withColumnRenamed('renamed_value', column)\n",
    "                            except:\n",
    "                                return Response({\"status\": \"validation\", \"error\": \"please select one column alone in both Groupby and Orderby\"})\n",
    "\n",
    "                    except:\n",
    "                        return Response({\"status\": \"failure\", \"error\": \"Replace unknown values into null values before selecting the forward/backward method\"})\n",
    "\n",
    "        return spark_df_filled\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return Response({\"status\": \"failure\", \"error\": str(e)})\n",
    "\n",
    "\n",
    "class fill_forward_backward(APIView):\n",
    "    def post(self,request):\n",
    "        #auth_keyword, token = get_authorization_header(request).split()\n",
    "        #token = token.decode(\"utf-8\")\n",
    "        #check = headercheck(token)\n",
    "        check ='ok'\n",
    "        if check == 'ok':\n",
    "            pass\n",
    "        else:\n",
    "            return Response({\"status\": \"validation\", \"error\": \"user has been killed\"})\n",
    "        data=request.data\n",
    "        #analysis = data['analysis_name']\n",
    "        #filename = data['filename']\n",
    "        #version = data['version']\n",
    "        #username = data['username']\n",
    "        df_path = data['path']\n",
    "        column = data['column']\n",
    "        method = data['method']\n",
    "        Groupby = data.get('Groupby_column','')\n",
    "        Orderby = data.get('Orderby_column','')\n",
    "        try:\n",
    "            import numpy as np\n",
    "            #file = match_version(version, analysis, filename)\n",
    "            #data = read_file(file)  # spark data frame\n",
    "            #if str(type(data)) == \"<class 'str'>\":\n",
    "             #   data = read_file(filename)\n",
    "            #data = data.toPandas()\n",
    "            if Groupby =='None' or Groupby=='' or Groupby=='none':\n",
    "                Groupby=None\n",
    "            else:\n",
    "                Groupby=Groupby\n",
    "            if Orderby =='None' or Orderby=='' or Orderby=='none':\n",
    "                Orderby=None\n",
    "            else:\n",
    "                Orderby=Orderby\n",
    "\n",
    "            df = spark.read.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(df_path)\n",
    "            df1 = fillemptycells(df=df, column=column, method=method, Groupby=Groupby, Orderby=Orderby)\n",
    "            results =df1.toJSON().collect()\n",
    "            return Response({'status':'success','data':results})\n",
    "        except Exception as e:\n",
    "            traceback.print_exc()\n",
    "            return Response({\"status\": \"failure\", \"error\": \"Please refer the logs in Logs->Container Log\"})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
