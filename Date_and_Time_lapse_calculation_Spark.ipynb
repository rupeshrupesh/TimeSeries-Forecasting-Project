{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-11T18:28:36.758947Z",
     "start_time": "2020-08-11T18:28:34.285016Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "spark_home = \"/opt/spark\"\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"python3\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"]=\"jupyter\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON_OPTS\"]=\"notebook\"\n",
    "os.environ[\"SPARK_HOME\"] = spark_home\n",
    "os.environ[\"SPARK_CLASSPATH\"] = \"/opt/spark/jars/sqljdbc4.jar\"\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"DateTimeLapse_Spark\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-11T18:28:39.277585Z",
     "start_time": "2020-08-11T18:28:39.250189Z"
    }
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "import traceback\n",
    "\n",
    "# from pyspark.sql import SQLContext\n",
    "# sqlContext = pyspark.SQLContext(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-11T18:29:22.964543Z",
     "start_time": "2020-08-11T18:29:22.944151Z"
    }
   },
   "outputs": [],
   "source": [
    "def DateTimeLapse_Spark (*, dataframe, end_col, end_format, start_col, start_format) :\n",
    "    \n",
    "    \"\"\"Calculates the difference of days and time lapsed between two datetime or timestamp columns\n",
    "\n",
    "    Note:\n",
    "        Specify formats according to datetime pattern\n",
    "        (https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html)\n",
    "\n",
    "    Args:\n",
    "        dataframe(DataFrame of str): A DataFrame consisting any two datetime or timestamp columns\n",
    "        end_col  (Column): A Column of datetime or timestamp datatype\n",
    "        end_format(str)  : Format of end_col\n",
    "        start_col(Column): Another Column of datetime or timestamp datatype\n",
    "        start_format(str): Format of start_col\n",
    "\n",
    "    Returns:\n",
    "        DataFrame of (str, str, double, double, double, bigint): A DataFrame containing start_col, end_col, 'diff_days', 'diff_hrs', 'diff_mins' and 'diff_secs' columns\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        df = dataframe\n",
    "        end_col = str(end_col)\n",
    "        end_format = str(end_format)\n",
    "        start_col = str(start_col)\n",
    "        start_format = str(start_format)\n",
    "        \n",
    "        spec_format = \"yyyy-MM-dd HH:mm:ss\"\n",
    "        \n",
    "        df = df.withColumn('endCol', when(to_date(col(end_col),end_format).isNotNull(), date_format(to_date(col(end_col),end_format), spec_format)))\n",
    "        df = df.withColumn('startCol', when(to_date(col(start_col),start_format).isNotNull(), date_format(to_date(col(start_col),start_format), spec_format)))\n",
    "        # df = df.withColumn('diff_days', datediff(col(end_col), col(start_col)))\n",
    "        \n",
    "        df = df.withColumn(start_col+'_unix', unix_timestamp(col('startCol'), format=spec_format))\n",
    "        df = df.withColumn(end_col+'_unix', unix_timestamp(col('endCol'), format=spec_format))\n",
    "        diff_secs_col = col(end_col+'_unix') - col(start_col+'_unix')\n",
    "        df = df.withColumn(\"diff_days\", diff_secs_col/(3600*24))\n",
    "        df = df.withColumn(\"diff_hrs\", diff_secs_col/3600)\n",
    "        df = df.withColumn(\"diff_mins\", diff_secs_col/60)\n",
    "        df = df.withColumn(\"diff_secs\", diff_secs_col)\n",
    "        df = df.drop(end_col+'_unix',start_col+'_unix','startCol','endCol')\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        traceback.print_exc()\n",
    "        return str(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-11T18:30:39.172862Z",
     "start_time": "2020-08-11T18:30:38.772327Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+------------+\n",
      "|       Employee Name|       DOB|Date of Hire|\n",
      "+--------------------+----------+------------+\n",
      "|          Brown, Mia|11/24/1985|  10/27/2008|\n",
      "|LaRotonda, William  | 4/26/1984|    1/6/2014|\n",
      "|    Steans, Tyrone  |  9/1/1986|   9/29/2014|\n",
      "|     Howard, Estelle| 9/16/1985|   2/16/2015|\n",
      "|         Singh, Nan | 5/19/1988|    5/1/2015|\n",
      "+--------------------+----------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Loading spark dataframe as 'df'\n",
    "df_path = \"/home/hduser/SVM_Projects/Datasets/core_dataset.csv\"\n",
    "df = spark.read.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(df_path)\n",
    "df = df.select('Employee Name','DOB','Date of Hire')\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-11T18:30:56.637712Z",
     "start_time": "2020-08-11T18:30:56.229987Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+------------+---------+--------+----------+---------+\n",
      "|       Employee Name|       DOB|Date of Hire|diff_days|diff_hrs| diff_mins|diff_secs|\n",
      "+--------------------+----------+------------+---------+--------+----------+---------+\n",
      "|          Brown, Mia|11/24/1985|  10/27/2008|   8373.0|200952.0|1.205712E7|723427200|\n",
      "|LaRotonda, William  | 4/26/1984|    1/6/2014|  10847.0|260328.0|1.561968E7|937180800|\n",
      "|    Steans, Tyrone  |  9/1/1986|   9/29/2014|  10255.0|246120.0| 1.47672E7|886032000|\n",
      "|     Howard, Estelle| 9/16/1985|   2/16/2015|  10745.0|257880.0| 1.54728E7|928368000|\n",
      "|         Singh, Nan | 5/19/1988|    5/1/2015|   9843.0|236232.0|1.417392E7|850435200|\n",
      "+--------------------+----------+------------+---------+--------+----------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Employee Name', 'string'),\n",
       " ('DOB', 'string'),\n",
       " ('Date of Hire', 'string'),\n",
       " ('diff_days', 'double'),\n",
       " ('diff_hrs', 'double'),\n",
       " ('diff_mins', 'double'),\n",
       " ('diff_secs', 'bigint')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dff = DateTimeLapse_Spark(dataframe=df,\n",
    "                          end_col=\"Date of Hire\",end_format=\"MM/dd/yyyy\",\n",
    "                          start_col=\"DOB\",start_format=\"MM/dd/yyyy\")\n",
    "dff.show(5)\n",
    "dff.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
