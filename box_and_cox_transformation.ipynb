{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "SPARK_HOME=\"/opt/spark\"\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"python3\"\n",
    "# os.environ[\"PYSPARK_PYTHON\"]=\"python\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"]=\"jupyter\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON_OPTS\"]=\"notebook\"\n",
    "os.environ[\"SPARK_HOME\"] = SPARK_HOME\n",
    "os.environ[\"SPARK_CLASSPATH\"] = \"/opt/spark/jars/sqljdbc4.jar\"\n",
    "\n",
    " \n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"Task_3\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pyspark.sql import SQLContext\n",
    "# sqlContext = SQLContext(spark)\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from scipy.stats import boxcox\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Money: double (nullable = true)\n",
      " |-- Spending: double (nullable = true)\n",
      " |-- Gain: integer (nullable = true)\n",
      " |-- Loss: integer (nullable = true)\n",
      "\n",
      "+----------+------+--------+----+----+\n",
      "|      Date| Money|Spending|Gain|Loss|\n",
      "+----------+------+--------+----+----+\n",
      "|1995-01-01|3492.4|  4851.2|2891|1000|\n",
      "|1995-02-01|3489.9|  4850.8|2893|1101|\n",
      "|1995-03-01|3491.1|  4885.4|2895|1202|\n",
      "|1995-04-01|3499.2|  4890.2|2897|1303|\n",
      "|1995-05-01|3524.2|  4933.1|   0|1404|\n",
      "|1995-06-01|3548.9|  4977.5|   0|1505|\n",
      "|1995-07-01|3567.4|  4970.2|   0|1606|\n",
      "|1995-08-01|3589.0|  5005.3|   0|1707|\n",
      "|1995-09-01|3602.1|  5020.5|   0|1808|\n",
      "|1995-10-01|3613.4|  5013.9|   0|1909|\n",
      "|1995-11-01|3619.9|  5055.6|   0|2010|\n",
      "|1995-12-01|3629.5|  5097.5|   0|2111|\n",
      "|1996-01-01|3647.9|  5085.7|   0|2212|\n",
      "|1996-02-01|3661.9|  5132.8|2917|2313|\n",
      "|1996-03-01|3687.0|  5173.3|2919|2414|\n",
      "|1996-04-01|3697.8|  5208.0|2921|   0|\n",
      "|1996-05-01|3709.7|  5223.8|2923|   0|\n",
      "|1996-06-01|3722.7|  5229.8|2925|   0|\n",
      "|1996-07-01|3737.3|  5251.9|2927|   0|\n",
      "|1996-08-01|3744.3|  5275.0|2929|   0|\n",
      "+----------+------+--------+----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_path = \"/home/rupeshr/Desktop/TSA_Python/dataset/multivariate_dataset.csv\"\n",
    "df =spark.read.format(\"com.databricks.spark.csv\")\\\n",
    "                    .option(\"multiline\", \"true\")\\\n",
    "                    .option(\"header\", \"true\")\\\n",
    "                    .option(\"inferSchema\", \"true\")\\\n",
    "                    .option(\"sep\", ',')\\\n",
    "                    .option('escape', '\\\"')\\\n",
    "                    .option(\"allowSingleQuotes\", \"true\")\\\n",
    "                    .option(\"ignoreLeadingWhiteSpace\", \"true\")\\\n",
    "                    .option(\"ignoreTrailingWhiteSpace\", \"true\")\\\n",
    "                    .load(df_path)\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Window\n",
    "from rest_framework.response import Response\n",
    "# Auxiliary functions\n",
    "# Pandas Types -> Spark Types\n",
    "#from edge.stats.sparksession import spark_session\n",
    "\n",
    "def equivalent_type(f):\n",
    "    if f == 'datetime64[ns]': return DateType()\n",
    "    elif f == 'int64': return LongType()\n",
    "    elif f == 'int32': return IntegerType()\n",
    "    elif f == 'float64': return DoubleType()\n",
    "    else: return StringType()\n",
    "def define_structure(string, format_type):\n",
    "    try: typo = equivalent_type(format_type)\n",
    "    except: typo = StringType()\n",
    "    return StructField(string, typo)\n",
    "# Converts pandas dataframe to spark dataframe\n",
    "def pandas_to_spark(df_pandas):\n",
    "    sqlContext = spark\n",
    "    columns = list(df_pandas.columns)\n",
    "    types = list(df_pandas.dtypes)\n",
    "    struct_list = []\n",
    "    for column, typo in zip(columns, types):\n",
    "        struct_list.append(define_structure(column, typo))\n",
    "    p_schema = StructType(struct_list)\n",
    "    return sqlContext.createDataFrame(df_pandas, p_schema)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def box_and_cox(*,df,column,lambdavalue):\n",
    "    for i in df.columns:\n",
    "        if i in column:\n",
    "            s = df.select(i).dtypes[0]\n",
    "            try:\n",
    "                if s[1] == \"int\" or s[1] == \"float\" or s[1] == \"double\" or s[1] == \"bigint\" or s[1] == \"long\":\n",
    "                    temp_df=df.select(i).toPandas()\n",
    "                    temp_df[column]=boxcox(temp_df[column],lmbda=lambdavalue)\n",
    "                    data=pandas_to_spark(temp_df)\n",
    "            except:\n",
    "                return Response({\"status\": \"failure\", \"error\": \"please select numerical type column\"})\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "#Below are some common values for lambda\n",
    "\n",
    "#lambda = -1. is a reciprocal transform.\n",
    "#lambda = -0.5 is a reciprocal square root transform.\n",
    "#lambda = 0.0 is a log transform.\n",
    "#lambda = 0.5 is a square root transform.\n",
    "#lambda = 1.0 is no transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------------+\n",
      "|  Month|Monthly beer production|\n",
      "+-------+-----------------------+\n",
      "|1956-01|                   93.2|\n",
      "|1956-02|                   96.0|\n",
      "|1956-03|                   95.2|\n",
      "|1956-04|                   77.1|\n",
      "|1956-05|                   70.9|\n",
      "|1956-06|                   64.8|\n",
      "|1956-07|                   70.1|\n",
      "|1956-08|                   77.3|\n",
      "|1956-09|                   79.5|\n",
      "|1956-10|                  100.6|\n",
      "|1956-11|                  100.7|\n",
      "|1956-12|                  107.1|\n",
      "|1957-01|                   95.9|\n",
      "|1957-02|                   82.8|\n",
      "|1957-03|                   83.3|\n",
      "|1957-04|                   80.0|\n",
      "|1957-05|                   80.4|\n",
      "|1957-06|                   67.5|\n",
      "|1957-07|                   75.7|\n",
      "|1957-08|                   71.1|\n",
      "+-------+-----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_path = \"/home/rupeshr/Desktop/TSA_Python/dataset/time series dataset/archive/monthly-beer-production-in-austr.csv\"\n",
    "df = spark.read.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(df_path)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=box_and_cox(df=df,column='Monthly beer production',lambdavalue=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------------+\n",
      "|  Month|Monthly beer production|\n",
      "+-------+-----------------------+\n",
      "|1956-01|                   93.2|\n",
      "|1956-02|                   96.0|\n",
      "|1956-03|                   95.2|\n",
      "|1956-04|                   77.1|\n",
      "|1956-05|                   70.9|\n",
      "|1956-06|                   64.8|\n",
      "|1956-07|                   70.1|\n",
      "|1956-08|                   77.3|\n",
      "|1956-09|                   79.5|\n",
      "|1956-10|                  100.6|\n",
      "|1956-11|                  100.7|\n",
      "|1956-12|                  107.1|\n",
      "|1957-01|                   95.9|\n",
      "|1957-02|                   82.8|\n",
      "|1957-03|                   83.3|\n",
      "|1957-04|                   80.0|\n",
      "|1957-05|                   80.4|\n",
      "|1957-06|                   67.5|\n",
      "|1957-07|                   75.7|\n",
      "|1957-08|                   71.1|\n",
      "+-------+-----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "var=['column','spen']\n",
    "print(len(var))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
